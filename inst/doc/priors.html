<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8" />
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />

<meta name="viewport" content="width=device-width, initial-scale=1">

<meta name="author" content="Jonah Gabry and Ben Goodrich" />

<meta name="date" content="2018-11-08" />

<title>Prior Distributions for rstanarm Models</title>



<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>



<link href="data:text/css;charset=utf-8,body%20%7B%0Abackground%2Dcolor%3A%20%23fff%3B%0Amargin%3A%201em%20auto%3B%0Amax%2Dwidth%3A%20700px%3B%0Aoverflow%3A%20visible%3B%0Apadding%2Dleft%3A%202em%3B%0Apadding%2Dright%3A%202em%3B%0Afont%2Dfamily%3A%20%22Open%20Sans%22%2C%20%22Helvetica%20Neue%22%2C%20Helvetica%2C%20Arial%2C%20sans%2Dserif%3B%0Afont%2Dsize%3A%2014px%3B%0Aline%2Dheight%3A%201%2E35%3B%0A%7D%0A%23header%20%7B%0Atext%2Dalign%3A%20center%3B%0A%7D%0A%23TOC%20%7B%0Aclear%3A%20both%3B%0Amargin%3A%200%200%2010px%2010px%3B%0Apadding%3A%204px%3B%0Awidth%3A%20400px%3B%0Aborder%3A%201px%20solid%20%23CCCCCC%3B%0Aborder%2Dradius%3A%205px%3B%0Abackground%2Dcolor%3A%20%23f6f6f6%3B%0Afont%2Dsize%3A%2013px%3B%0Aline%2Dheight%3A%201%2E3%3B%0A%7D%0A%23TOC%20%2Etoctitle%20%7B%0Afont%2Dweight%3A%20bold%3B%0Afont%2Dsize%3A%2015px%3B%0Amargin%2Dleft%3A%205px%3B%0A%7D%0A%23TOC%20ul%20%7B%0Apadding%2Dleft%3A%2040px%3B%0Amargin%2Dleft%3A%20%2D1%2E5em%3B%0Amargin%2Dtop%3A%205px%3B%0Amargin%2Dbottom%3A%205px%3B%0A%7D%0A%23TOC%20ul%20ul%20%7B%0Amargin%2Dleft%3A%20%2D2em%3B%0A%7D%0A%23TOC%20li%20%7B%0Aline%2Dheight%3A%2016px%3B%0A%7D%0Atable%20%7B%0Amargin%3A%201em%20auto%3B%0Aborder%2Dwidth%3A%201px%3B%0Aborder%2Dcolor%3A%20%23DDDDDD%3B%0Aborder%2Dstyle%3A%20outset%3B%0Aborder%2Dcollapse%3A%20collapse%3B%0A%7D%0Atable%20th%20%7B%0Aborder%2Dwidth%3A%202px%3B%0Apadding%3A%205px%3B%0Aborder%2Dstyle%3A%20inset%3B%0A%7D%0Atable%20td%20%7B%0Aborder%2Dwidth%3A%201px%3B%0Aborder%2Dstyle%3A%20inset%3B%0Aline%2Dheight%3A%2018px%3B%0Apadding%3A%205px%205px%3B%0A%7D%0Atable%2C%20table%20th%2C%20table%20td%20%7B%0Aborder%2Dleft%2Dstyle%3A%20none%3B%0Aborder%2Dright%2Dstyle%3A%20none%3B%0A%7D%0Atable%20thead%2C%20table%20tr%2Eeven%20%7B%0Abackground%2Dcolor%3A%20%23f7f7f7%3B%0A%7D%0Ap%20%7B%0Amargin%3A%200%2E5em%200%3B%0A%7D%0Ablockquote%20%7B%0Abackground%2Dcolor%3A%20%23f6f6f6%3B%0Apadding%3A%200%2E25em%200%2E75em%3B%0A%7D%0Ahr%20%7B%0Aborder%2Dstyle%3A%20solid%3B%0Aborder%3A%20none%3B%0Aborder%2Dtop%3A%201px%20solid%20%23777%3B%0Amargin%3A%2028px%200%3B%0A%7D%0Adl%20%7B%0Amargin%2Dleft%3A%200%3B%0A%7D%0Adl%20dd%20%7B%0Amargin%2Dbottom%3A%2013px%3B%0Amargin%2Dleft%3A%2013px%3B%0A%7D%0Adl%20dt%20%7B%0Afont%2Dweight%3A%20bold%3B%0A%7D%0Aul%20%7B%0Amargin%2Dtop%3A%200%3B%0A%7D%0Aul%20li%20%7B%0Alist%2Dstyle%3A%20circle%20outside%3B%0A%7D%0Aul%20ul%20%7B%0Amargin%2Dbottom%3A%200%3B%0A%7D%0Apre%2C%20code%20%7B%0Abackground%2Dcolor%3A%20%23f7f7f7%3B%0Aborder%2Dradius%3A%203px%3B%0Acolor%3A%20%23333%3B%0Awhite%2Dspace%3A%20pre%2Dwrap%3B%20%0A%7D%0Apre%20%7B%0Aborder%2Dradius%3A%203px%3B%0Amargin%3A%205px%200px%2010px%200px%3B%0Apadding%3A%2010px%3B%0A%7D%0Apre%3Anot%28%5Bclass%5D%29%20%7B%0Abackground%2Dcolor%3A%20%23f7f7f7%3B%0A%7D%0Acode%20%7B%0Afont%2Dfamily%3A%20Consolas%2C%20Monaco%2C%20%27Courier%20New%27%2C%20monospace%3B%0Afont%2Dsize%3A%2085%25%3B%0A%7D%0Ap%20%3E%20code%2C%20li%20%3E%20code%20%7B%0Apadding%3A%202px%200px%3B%0A%7D%0Adiv%2Efigure%20%7B%0Atext%2Dalign%3A%20center%3B%0A%7D%0Aimg%20%7B%0Abackground%2Dcolor%3A%20%23FFFFFF%3B%0Apadding%3A%202px%3B%0Aborder%3A%201px%20solid%20%23DDDDDD%3B%0Aborder%2Dradius%3A%203px%3B%0Aborder%3A%201px%20solid%20%23CCCCCC%3B%0Amargin%3A%200%205px%3B%0A%7D%0Ah1%20%7B%0Amargin%2Dtop%3A%200%3B%0Afont%2Dsize%3A%2035px%3B%0Aline%2Dheight%3A%2040px%3B%0A%7D%0Ah2%20%7B%0Aborder%2Dbottom%3A%204px%20solid%20%23f7f7f7%3B%0Apadding%2Dtop%3A%2010px%3B%0Apadding%2Dbottom%3A%202px%3B%0Afont%2Dsize%3A%20145%25%3B%0A%7D%0Ah3%20%7B%0Aborder%2Dbottom%3A%202px%20solid%20%23f7f7f7%3B%0Apadding%2Dtop%3A%2010px%3B%0Afont%2Dsize%3A%20120%25%3B%0A%7D%0Ah4%20%7B%0Aborder%2Dbottom%3A%201px%20solid%20%23f7f7f7%3B%0Amargin%2Dleft%3A%208px%3B%0Afont%2Dsize%3A%20105%25%3B%0A%7D%0Ah5%2C%20h6%20%7B%0Aborder%2Dbottom%3A%201px%20solid%20%23ccc%3B%0Afont%2Dsize%3A%20105%25%3B%0A%7D%0Aa%20%7B%0Acolor%3A%20%230033dd%3B%0Atext%2Ddecoration%3A%20none%3B%0A%7D%0Aa%3Ahover%20%7B%0Acolor%3A%20%236666ff%3B%20%7D%0Aa%3Avisited%20%7B%0Acolor%3A%20%23800080%3B%20%7D%0Aa%3Avisited%3Ahover%20%7B%0Acolor%3A%20%23BB00BB%3B%20%7D%0Aa%5Bhref%5E%3D%22http%3A%22%5D%20%7B%0Atext%2Ddecoration%3A%20underline%3B%20%7D%0Aa%5Bhref%5E%3D%22https%3A%22%5D%20%7B%0Atext%2Ddecoration%3A%20underline%3B%20%7D%0A%0Acode%20%3E%20span%2Ekw%20%7B%20color%3A%20%23555%3B%20font%2Dweight%3A%20bold%3B%20%7D%20%0Acode%20%3E%20span%2Edt%20%7B%20color%3A%20%23902000%3B%20%7D%20%0Acode%20%3E%20span%2Edv%20%7B%20color%3A%20%2340a070%3B%20%7D%20%0Acode%20%3E%20span%2Ebn%20%7B%20color%3A%20%23d14%3B%20%7D%20%0Acode%20%3E%20span%2Efl%20%7B%20color%3A%20%23d14%3B%20%7D%20%0Acode%20%3E%20span%2Ech%20%7B%20color%3A%20%23d14%3B%20%7D%20%0Acode%20%3E%20span%2Est%20%7B%20color%3A%20%23d14%3B%20%7D%20%0Acode%20%3E%20span%2Eco%20%7B%20color%3A%20%23888888%3B%20font%2Dstyle%3A%20italic%3B%20%7D%20%0Acode%20%3E%20span%2Eot%20%7B%20color%3A%20%23007020%3B%20%7D%20%0Acode%20%3E%20span%2Eal%20%7B%20color%3A%20%23ff0000%3B%20font%2Dweight%3A%20bold%3B%20%7D%20%0Acode%20%3E%20span%2Efu%20%7B%20color%3A%20%23900%3B%20font%2Dweight%3A%20bold%3B%20%7D%20%20code%20%3E%20span%2Eer%20%7B%20color%3A%20%23a61717%3B%20background%2Dcolor%3A%20%23e3d2d2%3B%20%7D%20%0A" rel="stylesheet" type="text/css" />

</head>

<body>




<h1 class="title toc-ignore">Prior Distributions for rstanarm Models</h1>
<h4 class="author"><em>Jonah Gabry and Ben Goodrich</em></h4>
<h4 class="date"><em>2018-11-08</em></h4>


<div id="TOC">
<ul>
<li><a href="#introduction">Introduction</a></li>
<li><a href="#default-weakly-informative-prior-distributions">Default (Weakly Informative) Prior Distributions</a><ul>
<li><a href="#automatic-prior-scale-adjustments">Automatic prior scale adjustments</a></li>
<li><a href="#disabling-prior-scale-adjustments">Disabling prior scale adjustments</a></li>
</ul></li>
<li><a href="#how-to-specify-flat-priors-and-why-you-typically-shouldnt">How to Specify Flat Priors (and why you typically shouldn’t)</a><ul>
<li><a href="#uninformative-is-usually-unwarranted-and-unrealistic-flat-is-frequently-frivolous-and-fictional">Uninformative is usually unwarranted and unrealistic (flat is frequently frivolous and fictional)</a></li>
<li><a href="#specifying-flat-priors">Specifying flat priors</a></li>
</ul></li>
<li><a href="#informative-prior-distributions">Informative Prior Distributions</a></li>
</ul>
</div>

<!--
%\VignetteEngine{knitr::rmarkdown}
%\VignetteIndexEntry{Prior Distributions}
-->
<div id="introduction" class="section level1">
<h1>Introduction</h1>
<p>This vignette provides an overview of how the specification of prior distributions works in the <strong>rstanarm</strong> package. It is still a work in progress and more content will be added in future versions of <strong>rstanarm</strong>. Before reading this vignette it is important to first read the <a href="rstanarm.html">How to Use the <strong>rstanarm</strong> Package</a> vignette, which provides a general overview of the package.</p>
<p>Every modeling function in <strong>rstanarm</strong> offers a subset of the arguments in the table below which are used for specifying prior distributions for the model parameters.</p>
<p><br></p>
<table style="width:58%;">
<colgroup>
<col width="19%"></col>
<col width="19%"></col>
<col width="19%"></col>
</colgroup>
<thead>
<tr class="header">
<th>Argument</th>
<th>Used in</th>
<th>Applies to</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><code>prior_intercept</code></td>
<td>All modeling functions except <code>stan_polr</code> and <code>stan_nlmer</code></td>
<td>Model intercept, after centering predictors.</td>
</tr>
<tr class="even">
<td><code>prior</code></td>
<td>All modeling functions</td>
<td>Regression coefficients. Does <em>not</em> include coefficients that vary by group in a multilevel model (see <code>prior_covariance</code>).</td>
</tr>
<tr class="odd">
<td><code>prior_aux</code></td>
<td><code>stan_glm</code>*, <code>stan_glmer</code>*, <code>stan_gamm4</code>, <code>stan_nlmer</code></td>
<td>Auxiliary parameter, e.g. error SD (interpretation depends on the GLM).</td>
</tr>
<tr class="even">
<td><code>prior_covariance</code></td>
<td><code>stan_glmer</code>*, <code>stan_gamm4</code>, <code>stan_nlmer</code></td>
<td>Covariance matrices in multilevel models with varying slopes and intercepts. See the <a href="http://mc-stan.org/rstanarm/articles/glmer.html"><code>stan_glmer</code> vignette</a> for details on this prior.</td>
</tr>
</tbody>
</table>
<p>* <code>stan_glm</code> also implies <code>stan_glm.nb</code>. <code>stan_glmer</code> implies <code>stan_lmer</code> and <code>stan_glmer.nb</code>.</p>
<p><br></p>
<p>The <code>stan_polr</code>, <code>stan_betareg</code>, and <code>stan_gamm4</code> functions also provide additional arguments specific only to those models:</p>
<table style="width:58%;">
<colgroup>
<col width="19%"></col>
<col width="19%"></col>
<col width="19%"></col>
</colgroup>
<thead>
<tr class="header">
<th>Argument</th>
<th>Used only in</th>
<th>Applies to</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><code>prior_smooth</code></td>
<td><code>stan_gamm4</code></td>
<td>Prior for hyperparameters in GAMs (lower values yield less flexible smooth functions).</td>
</tr>
<tr class="even">
<td><code>prior_counts</code></td>
<td><code>stan_polr</code></td>
<td>Prior counts of an <em>ordinal</em> outcome (when predictors at sample means).</td>
</tr>
<tr class="odd">
<td><code>prior_z</code></td>
<td><code>stan_betareg</code></td>
<td>Coefficients in the model for <code>phi</code>.</td>
</tr>
<tr class="even">
<td><code>prior_intercept_z</code></td>
<td><code>stan_betareg</code></td>
<td>Intercept in the model for <code>phi</code>.</td>
</tr>
<tr class="odd">
<td><code>prior_phi</code></td>
<td><code>stan_betareg</code></td>
<td><code>phi</code>, if not modeled as function of predictors.</td>
</tr>
</tbody>
</table>
<p><br></p>
<p>To specify these arguments the user provides a call to one of the various available functions for specifying priors (e.g., <code>prior = normal(0, 1)</code>, <code>prior = cauchy(c(0, 1), c(1, 2.5))</code>). The documentation for these functions can be found at <code>help(&quot;priors&quot;)</code>. The <strong>rstanarm</strong> documentation and the other <a href="index.html">vignettes</a> provide many examples of using these arguments to specify priors and the documentation for these arguments on the help pages for the various <strong>rstanarm</strong> modeling functions (e.g., <code>help(&quot;stan_glm&quot;)</code>) also explains which distributions can be used when specifying each of the prior-related arguments.</p>
<p><br></p>
</div>
<div id="default-weakly-informative-prior-distributions" class="section level1">
<h1>Default (Weakly Informative) Prior Distributions</h1>
<p>With very few exceptions, the default priors in <strong>rstanarm</strong> —the priors used if the arguments in the tables above are untouched— are <em>not</em> flat priors. Rather, the defaults are intended to be <em>weakly informative</em>. That is, they are designed to provide moderate regularization and help stabilize computation. For many (if not most) applications the defaults will perform well, but this is not guaranteed (there are no default priors that make sense for every possible model specification).</p>
<p>The way <strong>rstanarm</strong> attempts to make priors weakly informative by default is to internally adjust the scales of the priors. How this works (and, importantly, how to turn it off) is explained below, but first we can look at the default priors in action by fitting a basic linear regression model with the <code>stan_glm</code> function. For specifying priors, the <code>stan_glm</code> function accepts the arguments <code>prior_intercept</code>, <code>prior</code>, and <code>prior_aux</code>. To use the default priors we just leave those arguments at their defaults (i.e., we don’t specify them):</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(<span class="st">&quot;rstanarm&quot;</span>)
default_prior_test &lt;-<span class="st"> </span><span class="kw">stan_glm</span>(mpg ~<span class="st"> </span>wt +<span class="st"> </span>am, <span class="dt">data =</span> mtcars, <span class="dt">chains =</span> <span class="dv">1</span>)</code></pre></div>
<p>The <code>prior_summary</code> function provides a concise summary of the priors used:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">prior_summary</span>(default_prior_test)</code></pre></div>
<pre><code>Priors for model 'default_prior_test' 
------
Intercept (after predictors centered)
 ~ normal(location = 0, scale = 10)
     **adjusted scale = 60.27

Coefficients
 ~ normal(location = [0,0], scale = [2.5,2.5])
     **adjusted scale = [15.40,15.07]

Auxiliary (sigma)
 ~ exponential(rate = 1)
     **adjusted scale = 6.03 (adjusted rate = 1/adjusted scale)
------
See help('prior_summary.stanreg') for more details</code></pre>
<p>Starting from the bottom up, we can see that:</p>
<ul>
<li><p><strong>Auxiliary</strong>: <code>sigma</code>, the error standard deviation, has a default prior that is <span class="math inline">\(\mathsf{exponential}(1)\)</span>. However, as a result of the automatic rescaling, the actual scale used was 6.03.</p></li>
<li><p><strong>Coefficients</strong>: By default the regression coefficients (in this case the coefficients on the <code>wt</code> and <code>am</code> variables) are treated as a priori independent with normal priors centered at 0 and with scale (standard deviation) <span class="math inline">\(2.5\)</span>. Like for <code>sigma</code>, in order for the default to be weakly informative <strong>rstanarm</strong> will adjust the scales of the priors on the coefficients. As a result, the prior scales actually used were 15.40 and 15.07.</p></li>
<li><p><strong>Intercept</strong>: For the intercept, the default prior is normal with mean <span class="math inline">\(0\)</span> and standard deviation <span class="math inline">\(10\)</span>, but in this case the standard deviation was adjusted to 60.27. There is also a note in parentheses informing you that the prior applies to the intercept after all predictors have been centered (a similar note can be found in the documentation of the <code>prior_intercept</code> argument). In many cases the value of <span class="math inline">\(y\)</span> when <span class="math inline">\(x=0\)</span> is not meaningful and it is easier to think about the value when <span class="math inline">\(x = \bar{x}\)</span>. Therefore placing a prior on the intercept after centering the predictors typically makes it easier to specify a reasonable prior for the intercept. (Note: the user does <em>not</em> need to manually center the predictors.)</p></li>
</ul>
<p>To disable the centering of the predictors, you need to omit the intercept from the model <code>formula</code> and include a column of ones as a predictor (which cannot be named <code>&quot;(Intercept)&quot;</code> in the <code>data.frame</code>). Then you can specify a prior “coefficient” for the column of ones.</p>
<p>The next two subsections describe how the rescaling works and how to easily disable it if desired.</p>
<div id="automatic-prior-scale-adjustments" class="section level3">
<h3>Automatic prior scale adjustments</h3>
<p>For distributions that take an <code>autoscale</code> argument (see <code>help(&quot;priors&quot;)</code> for a list), if <code>autoscale</code> is left at <code>TRUE</code> (the default) then, in certain cases, the prior scales will be adjusted internally by <strong>rstanarm</strong>.</p>
<p>First, if the <em>outcome</em> <span class="math inline">\(y\)</span> is Gaussian, the prior scales for the intercept, coefficients, and error standard deviation are multiplied by a factor of <span class="math inline">\(\mathrm{sd}(y)\)</span>.</p>
<p>Additionally (not only for Gaussian models), if the <code>QR</code> argument to the model fitting function (e.g. <code>stan_glm</code>) is <code>FALSE</code> (the default) then:</p>
<ul>
<li>For a predictor <span class="math inline">\(x\)</span> with only one value nothing is changed.</li>
<li>For a predictor <span class="math inline">\(x\)</span> with exactly two unique values, we take the user-specified (or default) scale(s) for the selected priors and divide by the range of <span class="math inline">\(x\)</span>.</li>
<li>For a predictor <span class="math inline">\(x\)</span> with more than two unique values, we divide the prior scale(s) by <span class="math inline">\(\mathrm{sd}(x)\)</span>.</li>
</ul>
<p>Because the scaling is based on the scales of the predictors (and possibly the outcome) these are technically data-dependent priors. However, since these priors are quite wide (and in most cases rather conservative), the amount of information used is weak and mainly takes into account the order of magnitude of the variables. This enables <strong>rstanarm</strong> to offer defaults that are reasonable for many models.</p>
</div>
<div id="disabling-prior-scale-adjustments" class="section level3">
<h3>Disabling prior scale adjustments</h3>
<p>To disable automatic rescaling simply set the <code>autoscale</code> argument to to <code>FALSE</code>. For example:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">test_no_autoscale &lt;-
<span class="st">  </span><span class="kw">update</span>(
    default_prior_test,
    <span class="dt">prior =</span> <span class="kw">normal</span>(<span class="dv">0</span>, <span class="dv">5</span>, <span class="dt">autoscale =</span> <span class="ot">FALSE</span>),
    <span class="dt">prior_intercept =</span> <span class="kw">student_t</span>(<span class="dv">4</span>, <span class="dv">0</span>, <span class="dv">10</span>, <span class="dt">autoscale =</span> <span class="ot">FALSE</span>),
    <span class="dt">prior_aux =</span> <span class="kw">cauchy</span>(<span class="dv">0</span>, <span class="dv">3</span>, <span class="dt">autoscale =</span> <span class="ot">FALSE</span>)
  )</code></pre></div>
<p>We can verify that the prior scales weren’t adjusted by checking <code>prior_summary</code>:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">prior_summary</span>(test_no_autoscale)</code></pre></div>
<pre><code>Priors for model 'test_no_autoscale' 
------
Intercept (after predictors centered)
 ~ student_t(df = 4, location = 0, scale = 10)

Coefficients
 ~ normal(location = [0,0], scale = [5,5])

Auxiliary (sigma)
 ~ half-cauchy(location = 0, scale = 3)
------
See help('prior_summary.stanreg') for more details</code></pre>
<p>Disabling prior scale adjustments is usually unnecessary but is useful for when more informative prior information is available. There is an example of specifying an informative prior later in this vignette.</p>
<p><br></p>
</div>
</div>
<div id="how-to-specify-flat-priors-and-why-you-typically-shouldnt" class="section level1">
<h1>How to Specify Flat Priors (and why you typically shouldn’t)</h1>
<div id="uninformative-is-usually-unwarranted-and-unrealistic-flat-is-frequently-frivolous-and-fictional" class="section level3">
<h3>Uninformative is usually unwarranted and unrealistic (flat is frequently frivolous and fictional)</h3>
<p>When “non-informative” or “uninformative” is used in the context of prior distributions, it typically refers to a flat (uniform) distribution or a nearly flat distribution. Sometimes it may also be used to refer to the parameterization-invariant Jeffreys prior. Although <strong>rstanarm</strong> does not prevent you from using very diffuse or flat priors, unless the data is very strong it is wise to avoid them.</p>
<p>Rarely is it appropriate in any applied setting to use a prior that gives the same (or nearly the same) probability mass to values near zero as it gives values bigger than the age of the universe in nanoseconds. Even a much narrower prior than that, e.g., a normal distribution with <span class="math inline">\(\sigma = 500\)</span>, will tend to put much more probability mass on unreasonable parameter values than reasonable ones. In fact, using the prior <span class="math inline">\(\theta \sim \mathsf{Normal(\mu = 0, \sigma = 500)}\)</span> implies some strange prior beliefs. For example, you believe a priori that <span class="math inline">\(P(|\theta| &lt; 250) &lt; P(|\theta| &gt; 250)\)</span>, which can easily be verified by doing the calculation with the normal CDF</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">p &lt;-<span class="st"> </span><span class="dv">1</span> -<span class="st"> </span><span class="dv">2</span> *<span class="st"> </span><span class="kw">pnorm</span>(-<span class="dv">250</span>, <span class="dt">mean =</span> <span class="dv">0</span>, <span class="dt">sd =</span> <span class="dv">500</span>)
<span class="kw">print</span>(<span class="kw">paste</span>(<span class="st">&quot;Pr(-250 &lt; theta &lt; 250) =&quot;</span>, <span class="kw">round</span>(p, <span class="dv">2</span>)))</code></pre></div>
<pre><code>[1] &quot;Pr(-250 &lt; theta &lt; 250) = 0.38&quot;</code></pre>
<p>or via approximation with Monte Carlo draws:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">theta &lt;-<span class="st"> </span><span class="kw">rnorm</span>(<span class="fl">1e5</span>, <span class="dt">mean =</span> <span class="dv">0</span>, <span class="dt">sd =</span> <span class="dv">500</span>)
p_approx &lt;-<span class="st"> </span><span class="kw">mean</span>(<span class="kw">abs</span>(theta) &lt;<span class="st"> </span><span class="dv">250</span>)
<span class="kw">print</span>(<span class="kw">paste</span>(<span class="st">&quot;Pr(-250 &lt; theta &lt; 250) =&quot;</span>, <span class="kw">round</span>(p_approx, <span class="dv">2</span>)))</code></pre></div>
<pre><code>[1] &quot;Pr(-250 &lt; theta &lt; 250) = 0.38&quot;</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">d &lt;-<span class="st"> </span><span class="kw">data.frame</span>(theta, <span class="dt">clr =</span> <span class="kw">abs</span>(theta) &gt;<span class="st"> </span><span class="dv">250</span>)
<span class="kw">ggplot</span>(d, <span class="kw">aes</span>(<span class="dt">x =</span> theta, <span class="dt">fill =</span> clr)) +<span class="st"> </span>
<span class="st">  </span><span class="kw">geom_histogram</span>(<span class="dt">binwidth =</span> <span class="dv">5</span>, <span class="dt">show.legend =</span> <span class="ot">FALSE</span>) +<span class="st"> </span>
<span class="st">  </span><span class="kw">scale_y_continuous</span>(<span class="dt">name =</span> <span class="st">&quot;&quot;</span>, <span class="dt">labels =</span> <span class="ot">NULL</span>, <span class="dt">expand =</span> <span class="kw">c</span>(<span class="dv">0</span>,<span class="dv">0</span>)) +<span class="st"> </span>
<span class="st">  </span><span class="kw">scale_x_continuous</span>(<span class="dt">name =</span> <span class="kw">expression</span>(theta), <span class="dt">breaks =</span> <span class="kw">c</span>(-<span class="dv">1000</span>, -<span class="dv">250</span>, <span class="dv">250</span>, <span class="dv">1000</span>))</code></pre></div>
<div class="figure" style="text-align: center">
<img src="data:image/png;base64,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" alt="_There is much more probability mass outside the interval (-250, 250)._" width="60%" />
<p class="caption">
<em>There is much more probability mass outside the interval (-250, 250).</em>
</p>
</div>
<p><br> This will almost never correspond to the prior beliefs of a researcher about a parameter in a well-specified applied regression model and yet priors like <span class="math inline">\(\theta \sim \mathsf{Normal(\mu = 0, \sigma = 500)}\)</span> (and more extreme) remain quite popular.</p>
<p>Even when you know very little, a flat or very wide prior will almost never be the best approximation to your beliefs about the parameters in your model that you can express using <strong>rstanarm</strong> (or other software). <em>Some</em> amount of prior information will be available. For example, even if there is nothing to suggest a priori that a particular coefficient will be positive or negative, there is almost always enough information to suggest that different orders of magnitude are not equally likely. Making use of this information when setting a prior scale parameter is simple —one heuristic is to set the scale an order of magnitude bigger than you suspect it to be— and has the added benefit of helping to stabilize computations.</p>
<p>A more in-depth discussion of non-informative vs weakly informative priors is available in the case study <a href="http://mc-stan.org/users/documentation/case-studies/weakly_informative_shapes.html"><em>How the Shape of a Weakly Informative Prior Affects Inferences</em></a>.</p>
</div>
<div id="specifying-flat-priors" class="section level3">
<h3>Specifying flat priors</h3>
<p><strong>rstanarm</strong> will use flat priors if <code>NULL</code> is specified rather than a distribution. For example, to use a flat prior on regression coefficients you would specify <code>prior=NULL</code>:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">flat_prior_test &lt;-<span class="st"> </span><span class="kw">stan_glm</span>(mpg ~<span class="st"> </span>wt, <span class="dt">data =</span> mtcars, <span class="dt">prior =</span> <span class="ot">NULL</span>)</code></pre></div>
<p>In this case we let <strong>rstanarm</strong> use the default priors for the intercept and error standard deviation (we could change that if we wanted), but the coefficient on the <code>wt</code> variable will have a flat prior. To double check that indeed a flat prior was used for the coefficient on <code>wt</code> we can call <code>prior_summary</code>:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">prior_summary</span>(flat_prior_test)</code></pre></div>
<pre><code>Priors for model 'flat_prior_test' 
------
Intercept (after predictors centered)
 ~ normal(location = 0, scale = 10)
     **adjusted scale = 60.27

Coefficients
 ~ flat

Auxiliary (sigma)
 ~ exponential(rate = 1)
     **adjusted scale = 6.03 (adjusted rate = 1/adjusted scale)
------
See help('prior_summary.stanreg') for more details</code></pre>
<p><br></p>
</div>
</div>
<div id="informative-prior-distributions" class="section level1">
<h1>Informative Prior Distributions</h1>
<p>Although the default priors tend to work well, prudent use of more informative priors is encouraged. For example, suppose we have a linear regression model <span class="math display">\[y_i \sim \mathsf{Normal}\left(\alpha + \beta_1 x_{1,i} + \beta_2 x_{2,i}, \, \sigma\right)\]</span> and we have evidence (perhaps from previous research on the same topic) that approximately <span class="math inline">\(\beta_1 \in (-15, -5)\)</span> and <span class="math inline">\(\beta_2 \in (-1, 1)\)</span>. An example of an informative prior for <span class="math inline">\(\boldsymbol{\beta} = (\beta_1, \beta_2)'\)</span> could be</p>
<p><span class="math display">\[
\boldsymbol{\beta} \sim \mathsf{Normal} \left( 
  \begin{pmatrix} -10 \\ 0 \end{pmatrix},
  \begin{pmatrix} 5^2 &amp; 0 \\ 0 &amp; 2^2 \end{pmatrix}
\right),
\]</span> which sets the prior means at the midpoints of the intervals and then allows for some wiggle room on either side. If the data are highly informative about the parameter values (enough to overwhelm the prior) then this prior will yield similar results to a non-informative prior. But as the amount of data and/or the signal-to-noise ratio decrease, using a more informative prior becomes increasingly important.</p>
<p>If the variables <code>y</code>, <code>x1</code>, and <code>x2</code> are in the data frame <code>dat</code> then this model can be specified as</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">my_prior &lt;-<span class="st"> </span><span class="kw">normal</span>(<span class="dt">location =</span> <span class="kw">c</span>(-<span class="dv">10</span>, <span class="dv">0</span>), <span class="dt">scale =</span> <span class="kw">c</span>(<span class="dv">5</span>, <span class="dv">2</span>), <span class="dt">autoscale =</span> <span class="ot">FALSE</span>)
<span class="kw">stan_glm</span>(y ~<span class="st"> </span>x1 +<span class="st"> </span>x2, <span class="dt">data =</span> dat, <span class="dt">prior =</span> my_prior)</code></pre></div>
<p>We left the priors for the intercept and error standard deviation at their defaults, but informative priors can be specified for those parameters in an analogous manner.</p>
</div>



<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
