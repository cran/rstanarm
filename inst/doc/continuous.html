<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8" />
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />

<meta name="viewport" content="width=device-width, initial-scale=1">

<meta name="author" content="Jonah Gabry and Ben Goodrich" />

<meta name="date" content="2018-11-08" />

<title>Estimating Generalized Linear Models for Continuous Data with rstanarm</title>



<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>



<link href="data:text/css;charset=utf-8,body%20%7B%0Abackground%2Dcolor%3A%20%23fff%3B%0Amargin%3A%201em%20auto%3B%0Amax%2Dwidth%3A%20700px%3B%0Aoverflow%3A%20visible%3B%0Apadding%2Dleft%3A%202em%3B%0Apadding%2Dright%3A%202em%3B%0Afont%2Dfamily%3A%20%22Open%20Sans%22%2C%20%22Helvetica%20Neue%22%2C%20Helvetica%2C%20Arial%2C%20sans%2Dserif%3B%0Afont%2Dsize%3A%2014px%3B%0Aline%2Dheight%3A%201%2E35%3B%0A%7D%0A%23header%20%7B%0Atext%2Dalign%3A%20center%3B%0A%7D%0A%23TOC%20%7B%0Aclear%3A%20both%3B%0Amargin%3A%200%200%2010px%2010px%3B%0Apadding%3A%204px%3B%0Awidth%3A%20400px%3B%0Aborder%3A%201px%20solid%20%23CCCCCC%3B%0Aborder%2Dradius%3A%205px%3B%0Abackground%2Dcolor%3A%20%23f6f6f6%3B%0Afont%2Dsize%3A%2013px%3B%0Aline%2Dheight%3A%201%2E3%3B%0A%7D%0A%23TOC%20%2Etoctitle%20%7B%0Afont%2Dweight%3A%20bold%3B%0Afont%2Dsize%3A%2015px%3B%0Amargin%2Dleft%3A%205px%3B%0A%7D%0A%23TOC%20ul%20%7B%0Apadding%2Dleft%3A%2040px%3B%0Amargin%2Dleft%3A%20%2D1%2E5em%3B%0Amargin%2Dtop%3A%205px%3B%0Amargin%2Dbottom%3A%205px%3B%0A%7D%0A%23TOC%20ul%20ul%20%7B%0Amargin%2Dleft%3A%20%2D2em%3B%0A%7D%0A%23TOC%20li%20%7B%0Aline%2Dheight%3A%2016px%3B%0A%7D%0Atable%20%7B%0Amargin%3A%201em%20auto%3B%0Aborder%2Dwidth%3A%201px%3B%0Aborder%2Dcolor%3A%20%23DDDDDD%3B%0Aborder%2Dstyle%3A%20outset%3B%0Aborder%2Dcollapse%3A%20collapse%3B%0A%7D%0Atable%20th%20%7B%0Aborder%2Dwidth%3A%202px%3B%0Apadding%3A%205px%3B%0Aborder%2Dstyle%3A%20inset%3B%0A%7D%0Atable%20td%20%7B%0Aborder%2Dwidth%3A%201px%3B%0Aborder%2Dstyle%3A%20inset%3B%0Aline%2Dheight%3A%2018px%3B%0Apadding%3A%205px%205px%3B%0A%7D%0Atable%2C%20table%20th%2C%20table%20td%20%7B%0Aborder%2Dleft%2Dstyle%3A%20none%3B%0Aborder%2Dright%2Dstyle%3A%20none%3B%0A%7D%0Atable%20thead%2C%20table%20tr%2Eeven%20%7B%0Abackground%2Dcolor%3A%20%23f7f7f7%3B%0A%7D%0Ap%20%7B%0Amargin%3A%200%2E5em%200%3B%0A%7D%0Ablockquote%20%7B%0Abackground%2Dcolor%3A%20%23f6f6f6%3B%0Apadding%3A%200%2E25em%200%2E75em%3B%0A%7D%0Ahr%20%7B%0Aborder%2Dstyle%3A%20solid%3B%0Aborder%3A%20none%3B%0Aborder%2Dtop%3A%201px%20solid%20%23777%3B%0Amargin%3A%2028px%200%3B%0A%7D%0Adl%20%7B%0Amargin%2Dleft%3A%200%3B%0A%7D%0Adl%20dd%20%7B%0Amargin%2Dbottom%3A%2013px%3B%0Amargin%2Dleft%3A%2013px%3B%0A%7D%0Adl%20dt%20%7B%0Afont%2Dweight%3A%20bold%3B%0A%7D%0Aul%20%7B%0Amargin%2Dtop%3A%200%3B%0A%7D%0Aul%20li%20%7B%0Alist%2Dstyle%3A%20circle%20outside%3B%0A%7D%0Aul%20ul%20%7B%0Amargin%2Dbottom%3A%200%3B%0A%7D%0Apre%2C%20code%20%7B%0Abackground%2Dcolor%3A%20%23f7f7f7%3B%0Aborder%2Dradius%3A%203px%3B%0Acolor%3A%20%23333%3B%0Awhite%2Dspace%3A%20pre%2Dwrap%3B%20%0A%7D%0Apre%20%7B%0Aborder%2Dradius%3A%203px%3B%0Amargin%3A%205px%200px%2010px%200px%3B%0Apadding%3A%2010px%3B%0A%7D%0Apre%3Anot%28%5Bclass%5D%29%20%7B%0Abackground%2Dcolor%3A%20%23f7f7f7%3B%0A%7D%0Acode%20%7B%0Afont%2Dfamily%3A%20Consolas%2C%20Monaco%2C%20%27Courier%20New%27%2C%20monospace%3B%0Afont%2Dsize%3A%2085%25%3B%0A%7D%0Ap%20%3E%20code%2C%20li%20%3E%20code%20%7B%0Apadding%3A%202px%200px%3B%0A%7D%0Adiv%2Efigure%20%7B%0Atext%2Dalign%3A%20center%3B%0A%7D%0Aimg%20%7B%0Abackground%2Dcolor%3A%20%23FFFFFF%3B%0Apadding%3A%202px%3B%0Aborder%3A%201px%20solid%20%23DDDDDD%3B%0Aborder%2Dradius%3A%203px%3B%0Aborder%3A%201px%20solid%20%23CCCCCC%3B%0Amargin%3A%200%205px%3B%0A%7D%0Ah1%20%7B%0Amargin%2Dtop%3A%200%3B%0Afont%2Dsize%3A%2035px%3B%0Aline%2Dheight%3A%2040px%3B%0A%7D%0Ah2%20%7B%0Aborder%2Dbottom%3A%204px%20solid%20%23f7f7f7%3B%0Apadding%2Dtop%3A%2010px%3B%0Apadding%2Dbottom%3A%202px%3B%0Afont%2Dsize%3A%20145%25%3B%0A%7D%0Ah3%20%7B%0Aborder%2Dbottom%3A%202px%20solid%20%23f7f7f7%3B%0Apadding%2Dtop%3A%2010px%3B%0Afont%2Dsize%3A%20120%25%3B%0A%7D%0Ah4%20%7B%0Aborder%2Dbottom%3A%201px%20solid%20%23f7f7f7%3B%0Amargin%2Dleft%3A%208px%3B%0Afont%2Dsize%3A%20105%25%3B%0A%7D%0Ah5%2C%20h6%20%7B%0Aborder%2Dbottom%3A%201px%20solid%20%23ccc%3B%0Afont%2Dsize%3A%20105%25%3B%0A%7D%0Aa%20%7B%0Acolor%3A%20%230033dd%3B%0Atext%2Ddecoration%3A%20none%3B%0A%7D%0Aa%3Ahover%20%7B%0Acolor%3A%20%236666ff%3B%20%7D%0Aa%3Avisited%20%7B%0Acolor%3A%20%23800080%3B%20%7D%0Aa%3Avisited%3Ahover%20%7B%0Acolor%3A%20%23BB00BB%3B%20%7D%0Aa%5Bhref%5E%3D%22http%3A%22%5D%20%7B%0Atext%2Ddecoration%3A%20underline%3B%20%7D%0Aa%5Bhref%5E%3D%22https%3A%22%5D%20%7B%0Atext%2Ddecoration%3A%20underline%3B%20%7D%0A%0Acode%20%3E%20span%2Ekw%20%7B%20color%3A%20%23555%3B%20font%2Dweight%3A%20bold%3B%20%7D%20%0Acode%20%3E%20span%2Edt%20%7B%20color%3A%20%23902000%3B%20%7D%20%0Acode%20%3E%20span%2Edv%20%7B%20color%3A%20%2340a070%3B%20%7D%20%0Acode%20%3E%20span%2Ebn%20%7B%20color%3A%20%23d14%3B%20%7D%20%0Acode%20%3E%20span%2Efl%20%7B%20color%3A%20%23d14%3B%20%7D%20%0Acode%20%3E%20span%2Ech%20%7B%20color%3A%20%23d14%3B%20%7D%20%0Acode%20%3E%20span%2Est%20%7B%20color%3A%20%23d14%3B%20%7D%20%0Acode%20%3E%20span%2Eco%20%7B%20color%3A%20%23888888%3B%20font%2Dstyle%3A%20italic%3B%20%7D%20%0Acode%20%3E%20span%2Eot%20%7B%20color%3A%20%23007020%3B%20%7D%20%0Acode%20%3E%20span%2Eal%20%7B%20color%3A%20%23ff0000%3B%20font%2Dweight%3A%20bold%3B%20%7D%20%0Acode%20%3E%20span%2Efu%20%7B%20color%3A%20%23900%3B%20font%2Dweight%3A%20bold%3B%20%7D%20%20code%20%3E%20span%2Eer%20%7B%20color%3A%20%23a61717%3B%20background%2Dcolor%3A%20%23e3d2d2%3B%20%7D%20%0A" rel="stylesheet" type="text/css" />

</head>

<body>




<h1 class="title toc-ignore">Estimating Generalized Linear Models for Continuous Data with rstanarm</h1>
<h4 class="author"><em>Jonah Gabry and Ben Goodrich</em></h4>
<h4 class="date"><em>2018-11-08</em></h4>


<div id="TOC">
<ul>
<li><a href="#introduction">Introduction</a></li>
<li><a href="#likelihood">Likelihood</a></li>
<li><a href="#priors">Priors</a></li>
<li><a href="#posterior">Posterior</a></li>
<li><a href="#linear-regression-example">Linear Regression Example</a><ul>
<li><a href="#model-comparison">Model comparison</a></li>
<li><a href="#the-posterior-predictive-distribution">The posterior predictive distribution</a><ul>
<li><a href="#graphical-posterior-predictive-checks">Graphical posterior predictive checks</a></li>
<li><a href="#generating-predictions">Generating predictions</a></li>
</ul></li>
</ul></li>
<li><a href="#gamma-regression-example">Gamma Regression Example</a></li>
<li><a href="#references">References</a></li>
</ul>
</div>

<!--
%\VignetteEngine{knitr::rmarkdown}
%\VignetteIndexEntry{stan_glm: GLMs for Continuous Data}
-->
<div id="introduction" class="section level1">
<h1>Introduction</h1>
<p>This vignette explains how to estimate linear and generalized linear models (GLMs) for continuous response variables using the <code>stan_glm</code> function in the <strong>rstanarm</strong> package. For GLMs for discrete outcomes see the vignettes for <a href="binomial.html">binary/binomial</a> and <a href="count.html">count</a> outcomes.</p>
<p>This vignette primarily focuses on Steps 1 and 2 when the likelihood is the product of conditionally independent continuous distributions. Steps 3 and 4 are covered in more depth by the vignette entitled <a href="rstanarm.html">“How to Use the <strong>rstanarm</strong> Package”</a>, although this vignette does also give a few examples of model checking and generating predictions.</p>
</div>
<div id="likelihood" class="section level1">
<h1>Likelihood</h1>
<p>In the simplest case a GLM for a continuous outcome is simply a linear model and the likelihood for one observation is a conditionally normal PDF <span class="math display">\[\frac{1}{\sigma \sqrt{2 \pi}} e^{-\frac{1}{2} 
\left(\frac{y - \mu}{\sigma}\right)^2},\]</span> where <span class="math inline">\(\mu = \alpha + \mathbf{x}^\top \boldsymbol{\beta}\)</span> is a linear predictor and <span class="math inline">\(\sigma\)</span> is the standard deviation of the error in predicting the outcome, <span class="math inline">\(y\)</span>.</p>
<p>More generally, a linear predictor <span class="math inline">\(\eta = \alpha + \mathbf{x}^\top \boldsymbol{\beta}\)</span> can be related to the conditional mean of the outcome via a link function <span class="math inline">\(g\)</span> that serves as a map between the range of values on which the outcome is defined and the space on which the linear predictor is defined. For the linear model described above no transformation is needed and so the link function is taken to be the identity function. However, there are cases in which a link function is used for Gaussian models; the log link, for example, can be used to log transform the (conditional) expected value of the outcome when it is constrained to be positive.</p>
<p>Like the <code>glm</code> function, the <code>stan_glm</code> function uses R’s family objects. The family objects for continuous outcomes compatible with <code>stan_glm</code> are the <code>gaussian</code>, <code>Gamma</code>, and <code>inverse.gaussian</code> distributions. All of the link functions provided by these family objects are also compatible with <code>stan_glm</code>. For example, for a Gamma GLM, where we assume that observations are conditionally independent Gamma random variables, common link functions are the log and inverse links.</p>
<p>Regardless of the distribution and link function, the likelihood for the entire sample is the product of the likelihood contributions of the individual observations.</p>
</div>
<div id="priors" class="section level1">
<h1>Priors</h1>
</div>
<div id="posterior" class="section level1">
<h1>Posterior</h1>
<p>With independent prior distributions, the joint posterior distribution for <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\boldsymbol{\beta}\)</span> is proportional to the product of the priors and the <span class="math inline">\(N\)</span> likelihood contributions:</p>
<p><span class="math display">\[f\left(\boldsymbol{\beta} | \mathbf{y},\mathbf{X}\right) \propto
  f\left(\alpha\right) \times \prod_{k=1}^K f\left(\beta_k\right) \times
  \prod_{i=1}^N {f(y_i|\eta_i)},\]</span></p>
<p>where <span class="math inline">\(\mathbf{X}\)</span> is the matrix of predictors and <span class="math inline">\(\eta\)</span> the linear predictor. This is the posterior distribution that <code>stan_glm</code> will draw from when using MCMC.</p>
</div>
<div id="linear-regression-example" class="section level1">
<h1>Linear Regression Example</h1>
<p>The <code>stan_lm</code> function, which has its own <a href="lm.html">vignette</a>, fits regularized linear models using a novel means of specifying priors for the regression coefficients. Here we focus using the <code>stan_glm</code> function, which can be used to estimate linear models with independent priors on the regression coefficients.</p>
<p>To illustrate the usage of <code>stan_glm</code> and some of the post-processing functions in the <strong>rstanarm</strong> package we’ll use a simple example from Chapter 3 of <a href="http://www.stat.columbia.edu/~gelman/arm/">Gelman and Hill (2007)</a>:</p>
<blockquote>
<p>We shall fit a series of regressions predicting cognitive test scores of three- and four-year-old children given characteristics of their mothers, using data from a survey of adult American women and their children (a subsample from the National Longitudinal Survey of Youth).</p>
</blockquote>
<p>Using two predictors – a binary indicator for whether the mother has a high-school degree (<code>mom_hs</code>) and the mother’s score on an IQ test (<code>mom_iq</code>) – we will fit four contending models. The first two models will each use just one of the predictors, the third will use both, and the fourth will also include a term for the interaction of the two predictors.</p>
<p>For these models we’ll use the default weakly informative priors for <code>stan_glm</code>, which are currently set to <code>normal(0,10)</code> for the intercept and <code>normal(0,5)</code> for the other regression coefficients. For an overview of the many other available prior distributions see <code>help(&quot;prior&quot;, package = &quot;rstanarm&quot;)</code>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(rstanarm)
<span class="kw">data</span>(kidiq)
post1 &lt;-<span class="st"> </span><span class="kw">stan_glm</span>(kid_score ~<span class="st"> </span>mom_hs, <span class="dt">data =</span> kidiq, 
                  <span class="dt">family =</span> <span class="kw">gaussian</span>(<span class="dt">link =</span> <span class="st">&quot;identity&quot;</span>), 
                  <span class="dt">chains =</span> CHAINS, <span class="dt">cores =</span> CORES, <span class="dt">seed =</span> SEED, <span class="dt">iter =</span> ITER)
post2 &lt;-<span class="st"> </span><span class="kw">update</span>(post1, <span class="dt">formula =</span> . ~<span class="st"> </span>mom_iq)
post3 &lt;-<span class="st"> </span><span class="kw">update</span>(post1, <span class="dt">formula =</span> . ~<span class="st"> </span>mom_hs +<span class="st"> </span>mom_iq)
(post4 &lt;-<span class="st"> </span><span class="kw">update</span>(post1, <span class="dt">formula =</span> . ~<span class="st"> </span>mom_hs *<span class="st"> </span>mom_iq))</code></pre></div>
<p>Following Gelman and Hill’s example, we make some plots overlaying the estimated regression lines on the data.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">base &lt;-<span class="st"> </span><span class="kw">ggplot</span>(kidiq, <span class="kw">aes</span>(<span class="dt">x =</span> mom_hs, <span class="dt">y =</span> kid_score)) +<span class="st"> </span>
<span class="st">  </span><span class="kw">geom_point</span>(<span class="dt">size =</span> <span class="dv">1</span>, <span class="dt">position =</span> <span class="kw">position_jitter</span>(<span class="dt">height =</span> <span class="fl">0.05</span>, <span class="dt">width =</span> <span class="fl">0.1</span>)) +<span class="st"> </span>
<span class="st">  </span><span class="kw">scale_x_continuous</span>(<span class="dt">breaks =</span> <span class="kw">c</span>(<span class="dv">0</span>,<span class="dv">1</span>), <span class="dt">labels =</span> <span class="kw">c</span>(<span class="st">&quot;No HS&quot;</span>, <span class="st">&quot;HS&quot;</span>))
  
base +<span class="st"> </span><span class="kw">geom_abline</span>(<span class="dt">intercept =</span> <span class="kw">coef</span>(post1)[<span class="dv">1</span>], <span class="dt">slope =</span> <span class="kw">coef</span>(post1)[<span class="dv">2</span>], 
                   <span class="dt">color =</span> <span class="st">&quot;skyblue4&quot;</span>, <span class="dt">size =</span> <span class="dv">1</span>)</code></pre></div>
<p>There several ways we could add the uncertainty in our estimates to the plot. One way is to also plot the estimated regression line at each draw from the posterior distribution. To do this we can extract the posterior draws from the fitted model object using the <code>as.matrix</code> or <code>as.data.frame</code> methods:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">draws &lt;-<span class="st"> </span><span class="kw">as.data.frame</span>(post1)
<span class="kw">colnames</span>(draws)[<span class="dv">1</span>:<span class="dv">2</span>] &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;a&quot;</span>, <span class="st">&quot;b&quot;</span>)

base +<span class="st"> </span>
<span class="st">  </span><span class="kw">geom_abline</span>(<span class="dt">data =</span> draws, <span class="kw">aes</span>(<span class="dt">intercept =</span> a, <span class="dt">slope =</span> b), 
              <span class="dt">color =</span> <span class="st">&quot;skyblue&quot;</span>, <span class="dt">size =</span> <span class="fl">0.2</span>, <span class="dt">alpha =</span> <span class="fl">0.25</span>) +<span class="st"> </span>
<span class="st">  </span><span class="kw">geom_abline</span>(<span class="dt">intercept =</span> <span class="kw">coef</span>(post1)[<span class="dv">1</span>], <span class="dt">slope =</span> <span class="kw">coef</span>(post1)[<span class="dv">2</span>], 
              <span class="dt">color =</span> <span class="st">&quot;skyblue4&quot;</span>, <span class="dt">size =</span> <span class="dv">1</span>)</code></pre></div>
<p>For the second model we can make the same plot but the x-axis will show the continuous predictor <code>mom_iq</code>:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">draws &lt;-<span class="st"> </span><span class="kw">as.data.frame</span>(<span class="kw">as.matrix</span>(post2))
<span class="kw">colnames</span>(draws)[<span class="dv">1</span>:<span class="dv">2</span>] &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;a&quot;</span>, <span class="st">&quot;b&quot;</span>)
<span class="kw">ggplot</span>(kidiq, <span class="kw">aes</span>(<span class="dt">x =</span> mom_iq, <span class="dt">y =</span> kid_score)) +<span class="st"> </span>
<span class="st">  </span><span class="kw">geom_point</span>(<span class="dt">size =</span> <span class="dv">1</span>) +
<span class="st">  </span><span class="kw">geom_abline</span>(<span class="dt">data =</span> draws, <span class="kw">aes</span>(<span class="dt">intercept =</span> a, <span class="dt">slope =</span> b), 
              <span class="dt">color =</span> <span class="st">&quot;skyblue&quot;</span>, <span class="dt">size =</span> <span class="fl">0.2</span>, <span class="dt">alpha =</span> <span class="fl">0.25</span>) +<span class="st"> </span>
<span class="st">  </span><span class="kw">geom_abline</span>(<span class="dt">intercept =</span> <span class="kw">coef</span>(post2)[<span class="dv">1</span>], <span class="dt">slope =</span> <span class="kw">coef</span>(post2)[<span class="dv">2</span>], 
              <span class="dt">color =</span> <span class="st">&quot;skyblue4&quot;</span>, <span class="dt">size =</span> <span class="dv">1</span>)</code></pre></div>
<p>For the third and fourth models, each of which uses both predictors, we can plot the continuous <code>mom_iq</code> on the x-axis and use color to indicate which points correspond to the different subpopulations defined by <code>mom_hs</code>. We also now plot two regression lines, one for each subpopulation:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">reg0 &lt;-<span class="st"> </span>function(x, ests) <span class="kw">cbind</span>(<span class="dv">1</span>, <span class="dv">0</span>, x) %*%<span class="st"> </span>ests 
reg1 &lt;-<span class="st"> </span>function(x, ests) <span class="kw">cbind</span>(<span class="dv">1</span>, <span class="dv">1</span>, x) %*%<span class="st"> </span>ests

args &lt;-<span class="st"> </span><span class="kw">list</span>(<span class="dt">ests =</span> <span class="kw">coef</span>(post3))
kidiq$clr &lt;-<span class="st"> </span><span class="kw">factor</span>(kidiq$mom_hs, <span class="dt">labels =</span> <span class="kw">c</span>(<span class="st">&quot;No HS&quot;</span>, <span class="st">&quot;HS&quot;</span>))
lgnd &lt;-<span class="st"> </span><span class="kw">guide_legend</span>(<span class="dt">title =</span> <span class="ot">NULL</span>)
base2 &lt;-<span class="st"> </span><span class="kw">ggplot</span>(kidiq, <span class="kw">aes</span>(<span class="dt">x =</span> mom_iq, <span class="dt">fill =</span> <span class="kw">relevel</span>(clr, <span class="dt">ref =</span> <span class="st">&quot;HS&quot;</span>))) +<span class="st"> </span>
<span class="st">  </span><span class="kw">geom_point</span>(<span class="kw">aes</span>(<span class="dt">y =</span> kid_score), <span class="dt">shape =</span> <span class="dv">21</span>, <span class="dt">stroke =</span> .<span class="dv">2</span>, <span class="dt">size =</span> <span class="dv">1</span>) +<span class="st"> </span>
<span class="st">  </span><span class="kw">guides</span>(<span class="dt">color =</span> lgnd, <span class="dt">fill =</span> lgnd) +<span class="st"> </span>
<span class="st">  </span><span class="kw">theme</span>(<span class="dt">legend.position =</span> <span class="st">&quot;right&quot;</span>)
base2 +<span class="st"> </span>
<span class="st">  </span><span class="kw">stat_function</span>(<span class="dt">fun =</span> reg0, <span class="dt">args =</span> args, <span class="kw">aes</span>(<span class="dt">color =</span> <span class="st">&quot;No HS&quot;</span>), <span class="dt">size =</span> <span class="fl">1.5</span>) +
<span class="st">  </span><span class="kw">stat_function</span>(<span class="dt">fun =</span> reg1, <span class="dt">args =</span> args, <span class="kw">aes</span>(<span class="dt">color =</span> <span class="st">&quot;HS&quot;</span>), <span class="dt">size =</span> <span class="fl">1.5</span>)</code></pre></div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">reg0 &lt;-<span class="st"> </span>function(x, ests) <span class="kw">cbind</span>(<span class="dv">1</span>, <span class="dv">0</span>, x, <span class="dv">0</span> *<span class="st"> </span>x) %*%<span class="st"> </span>ests 
reg1 &lt;-<span class="st"> </span>function(x, ests) <span class="kw">cbind</span>(<span class="dv">1</span>, <span class="dv">1</span>, x, <span class="dv">1</span> *<span class="st"> </span>x) %*%<span class="st"> </span>ests
args &lt;-<span class="st"> </span><span class="kw">list</span>(<span class="dt">ests =</span> <span class="kw">coef</span>(post4))
base2 +
<span class="st">  </span><span class="kw">stat_function</span>(<span class="dt">fun =</span> reg0, <span class="dt">args =</span> args, <span class="kw">aes</span>(<span class="dt">color =</span> <span class="st">&quot;No HS&quot;</span>), <span class="dt">size =</span> <span class="fl">1.5</span>) +<span class="st"> </span>
<span class="st">  </span><span class="kw">stat_function</span>(<span class="dt">fun =</span> reg1, <span class="dt">args =</span> args, <span class="kw">aes</span>(<span class="dt">color =</span> <span class="st">&quot;HS&quot;</span>), <span class="dt">size =</span> <span class="fl">1.5</span>)</code></pre></div>
<div id="model-comparison" class="section level2">
<h2>Model comparison</h2>
<p>One way we can compare the four contending models is to use an approximation to Leave-One-Out (LOO) cross-validation, which is implemented by the <code>loo</code> function in the <strong>loo</strong> package:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Compare them with loo</span>
loo1 &lt;-<span class="st"> </span><span class="kw">loo</span>(post1)
loo2 &lt;-<span class="st"> </span><span class="kw">loo</span>(post2)
loo3 &lt;-<span class="st"> </span><span class="kw">loo</span>(post3)
loo4 &lt;-<span class="st"> </span><span class="kw">loo</span>(post4)
(comp &lt;-<span class="st"> </span><span class="kw">compare_models</span>(loo1, loo2, loo3, loo4))</code></pre></div>
<p>In this case the fourth model is preferred as it has the highest expected log predicted density (<code>elpd_loo</code>) or, equivalently, the lowest value of the LOO Information Criterion (<code>looic</code>). The fourth model is preferred by a lot over the first model</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">compare_models</span>(loo1, loo4)</code></pre></div>
<p>because the difference in <code>elpd</code> is so much larger than the standard error. However, the preference of the fourth model over the others isn’t as strong:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">compare_models</span>(loo3, loo4)
<span class="kw">compare_models</span>(loo2, loo4)</code></pre></div>
</div>
<div id="the-posterior-predictive-distribution" class="section level2">
<h2>The posterior predictive distribution</h2>
<p>The posterior predictive distribution is the distribution of the outcome implied by the model after using the observed data to update our beliefs about the unknown parameters. When simulating observations from the posterior predictive distribution we use the notation <span class="math inline">\(y^{\rm rep}\)</span> (for <em>replicate</em>) when we use the same observations of <span class="math inline">\(X\)</span> that were used to estimate the model parameters. When <span class="math inline">\(X\)</span> contains new observations we use the notation <span class="math inline">\(\tilde{y}\)</span> to refer to the posterior predictive simulations.</p>
<p>Simulating data from the posterior predictive distribution using the observed predictors is useful for checking the fit of the model. Drawing from the posterior predictive distribution at interesting values of the predictors also lets us visualize how a manipulation of a predictor affects (a function of) the outcome(s).</p>
<div id="graphical-posterior-predictive-checks" class="section level3">
<h3>Graphical posterior predictive checks</h3>
<p>The <code>pp_check</code> function generates a variety of plots comparing the observed outcome <span class="math inline">\(y\)</span> to simulated datasets <span class="math inline">\(y^{\rm rep}\)</span> from the posterior predictive distribution using the same observations of the predictors <span class="math inline">\(X\)</span> as we used to fit the model. He we show a few of the possible displays. The documentation at <code>help(&quot;pp_check.stanreg&quot;, package = &quot;rstanarm&quot;)</code> has details on all of the <code>pp_check</code> options.</p>
<p>First we’ll look at a plot directly comparing the distributions of <span class="math inline">\(y\)</span> and <span class="math inline">\(y^{\rm rep}\)</span>. The following call to <code>pp_check</code> will create a plot juxtaposing the histogram of <span class="math inline">\(y\)</span> and histograms of five <span class="math inline">\(y^{\rm rep}\)</span> datasets:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">pp_check</span>(post4, <span class="dt">plotfun =</span> <span class="st">&quot;hist&quot;</span>, <span class="dt">nreps =</span> <span class="dv">5</span>)</code></pre></div>
<p>The idea is that if the model is a good fit to the data we should be able to generate data <span class="math inline">\(y^{\rm rep}\)</span> from the posterior predictive distribution that looks a lot like the observed data <span class="math inline">\(y\)</span>. That is, given <span class="math inline">\(y\)</span>, the <span class="math inline">\(y^{\rm rep}\)</span> we generate should be plausible.</p>
<p>Another useful plot we can make using <code>pp_check</code> shows the distribution of a test quantity <span class="math inline">\(T(y^{\rm rep})\)</span> compared to <span class="math inline">\(T(y)\)</span>, the value of the quantity in the observed data. When the argument <code>plotfun = &quot;stat&quot;</code> is specified, <code>pp_check</code> will simulate <span class="math inline">\(S\)</span> datasets <span class="math inline">\(y_1^{\rm rep}, \dots, y_S^{\rm rep}\)</span>, each containing <span class="math inline">\(N\)</span> observations. Here <span class="math inline">\(S\)</span> is the size of the posterior sample (the number of MCMC draws from the posterior distribution of the model parameters) and <span class="math inline">\(N\)</span> is the length of <span class="math inline">\(y\)</span>. We can then check if <span class="math inline">\(T(y)\)</span> is consistent with the distribution of <span class="math inline">\(\left(T(y_1^{\rm yep}), \dots, T(y_S^{\rm yep})\right)\)</span>. In the plot below we see that the mean of the observations is plausible when compared to the distribution of the means of the <span class="math inline">\(S\)</span> <span class="math inline">\(y^{\rm rep}\)</span> datasets:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">pp_check</span>(post4, <span class="dt">plotfun =</span> <span class="st">&quot;stat&quot;</span>, <span class="dt">stat =</span> <span class="st">&quot;mean&quot;</span>)</code></pre></div>
<p>Using <code>plotfun=&quot;stat_2d&quot;</code> we can also specify two test quantities and look at a scatterplot:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">pp_check</span>(post4, <span class="dt">plotfun =</span> <span class="st">&quot;stat_2d&quot;</span>, <span class="dt">stat =</span> <span class="kw">c</span>(<span class="st">&quot;mean&quot;</span>, <span class="st">&quot;sd&quot;</span>))</code></pre></div>
</div>
<div id="generating-predictions" class="section level3">
<h3>Generating predictions</h3>
<p>The <code>posterior_predict</code> function is used to generate replicated data <span class="math inline">\(y^{\rm rep}\)</span> or predictions for future observations <span class="math inline">\(\tilde{y}\)</span>. Here we show how to use <code>posterior_predict</code> to generate predictions of the outcome <code>kid_score</code> for a range of different values of <code>mom_iq</code> and for both subpopulations defined by <code>mom_hs</code>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">IQ_SEQ &lt;-<span class="st"> </span><span class="kw">seq</span>(<span class="dt">from =</span> <span class="dv">75</span>, <span class="dt">to =</span> <span class="dv">135</span>, <span class="dt">by =</span> <span class="dv">5</span>)
y_nohs &lt;-<span class="st"> </span><span class="kw">posterior_predict</span>(post4, <span class="dt">newdata =</span> <span class="kw">data.frame</span>(<span class="dt">mom_hs =</span> <span class="dv">0</span>, <span class="dt">mom_iq =</span> IQ_SEQ))
y_hs &lt;-<span class="st"> </span><span class="kw">posterior_predict</span>(post4, <span class="dt">newdata =</span> <span class="kw">data.frame</span>(<span class="dt">mom_hs =</span> <span class="dv">1</span>, <span class="dt">mom_iq =</span> IQ_SEQ))
<span class="kw">dim</span>(y_hs)</code></pre></div>
<p>We now have two matrices, <code>y_nohs</code> and <code>y_hs</code>. Each matrix has as many columns as there are values of <code>IQ_SEQ</code> and as many rows as the size of the posterior sample. One way to show the predictors is to plot the predictions for the two groups of kids side by side:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">par</span>(<span class="dt">mfrow =</span> <span class="kw">c</span>(<span class="dv">1</span>:<span class="dv">2</span>), <span class="dt">mar =</span> <span class="kw">c</span>(<span class="dv">5</span>,<span class="dv">4</span>,<span class="dv">2</span>,<span class="dv">1</span>))
<span class="kw">boxplot</span>(y_hs, <span class="dt">axes =</span> <span class="ot">FALSE</span>, <span class="dt">outline =</span> <span class="ot">FALSE</span>, <span class="dt">ylim =</span> <span class="kw">c</span>(<span class="dv">10</span>,<span class="dv">170</span>),
        <span class="dt">xlab =</span> <span class="st">&quot;Mom IQ&quot;</span>, <span class="dt">ylab =</span> <span class="st">&quot;Predicted Kid IQ&quot;</span>, <span class="dt">main =</span> <span class="st">&quot;Mom HS&quot;</span>)
<span class="kw">axis</span>(<span class="dv">1</span>, <span class="dt">at =</span> <span class="dv">1</span>:<span class="kw">ncol</span>(y_hs), <span class="dt">labels =</span> IQ_SEQ, <span class="dt">las =</span> <span class="dv">3</span>)
<span class="kw">axis</span>(<span class="dv">2</span>, <span class="dt">las =</span> <span class="dv">1</span>)
<span class="kw">boxplot</span>(y_nohs, <span class="dt">outline =</span> <span class="ot">FALSE</span>, <span class="dt">col =</span> <span class="st">&quot;red&quot;</span>, <span class="dt">axes =</span> <span class="ot">FALSE</span>, <span class="dt">ylim =</span> <span class="kw">c</span>(<span class="dv">10</span>,<span class="dv">170</span>),
        <span class="dt">xlab =</span> <span class="st">&quot;Mom IQ&quot;</span>, <span class="dt">ylab =</span> <span class="ot">NULL</span>, <span class="dt">main =</span> <span class="st">&quot;Mom No HS&quot;</span>)
<span class="kw">axis</span>(<span class="dv">1</span>, <span class="dt">at =</span> <span class="dv">1</span>:<span class="kw">ncol</span>(y_hs), <span class="dt">labels =</span> IQ_SEQ, <span class="dt">las =</span> <span class="dv">3</span>)</code></pre></div>
</div>
</div>
</div>
<div id="gamma-regression-example" class="section level1">
<h1>Gamma Regression Example</h1>
<p>Gamma regression is often used when the response variable is continuous and positive, and the <em>coefficient of variation</em> (rather than the variance) is constant.</p>
<p>We’ll use one of the standard examples of Gamma regression, which is taken from McCullagh &amp; Nelder (1989). This example is also given in the documentation for R’s <code>glm</code> function. The outcome of interest is the clotting time of blood (in seconds) for “normal plasma diluted to nine different percentage concentrations with prothrombin-free plasma; clotting was induced by two lots of thromboplastin” (p. 300).</p>
<p>The help page for R’s <code>glm</code> function presents the example as follows:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">clotting &lt;-<span class="st"> </span><span class="kw">data.frame</span>(
    <span class="dt">u =</span> <span class="kw">c</span>(<span class="dv">5</span>,<span class="dv">10</span>,<span class="dv">15</span>,<span class="dv">20</span>,<span class="dv">30</span>,<span class="dv">40</span>,<span class="dv">60</span>,<span class="dv">80</span>,<span class="dv">100</span>),
    <span class="dt">lot1 =</span> <span class="kw">c</span>(<span class="dv">118</span>,<span class="dv">58</span>,<span class="dv">42</span>,<span class="dv">35</span>,<span class="dv">27</span>,<span class="dv">25</span>,<span class="dv">21</span>,<span class="dv">19</span>,<span class="dv">18</span>),
    <span class="dt">lot2 =</span> <span class="kw">c</span>(<span class="dv">69</span>,<span class="dv">35</span>,<span class="dv">26</span>,<span class="dv">21</span>,<span class="dv">18</span>,<span class="dv">16</span>,<span class="dv">13</span>,<span class="dv">12</span>,<span class="dv">12</span>))
<span class="kw">summary</span>(<span class="kw">glm</span>(lot1 ~<span class="st"> </span><span class="kw">log</span>(u), <span class="dt">data =</span> clotting, <span class="dt">family =</span> Gamma))
<span class="kw">summary</span>(<span class="kw">glm</span>(lot2 ~<span class="st"> </span><span class="kw">log</span>(u), <span class="dt">data =</span> clotting, <span class="dt">family =</span> Gamma))</code></pre></div>
<p>To fit the analogous Bayesian models we can simply substitute <code>stan_glm</code> for <code>glm</code> above. However, instead of fitting separate models we can also reshape the data slightly and fit a model interacting lot with plasma concentration:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">clotting2 &lt;-<span class="st"> </span><span class="kw">with</span>(clotting, <span class="kw">data.frame</span>(
  <span class="dt">log_plasma =</span> <span class="kw">rep</span>(<span class="kw">log</span>(u), <span class="dv">2</span>),
  <span class="dt">clot_time =</span> <span class="kw">c</span>(lot1, lot2),
  <span class="dt">lot_id =</span> <span class="kw">factor</span>(<span class="kw">rep</span>(<span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">2</span>), <span class="dt">each =</span> <span class="kw">length</span>(u)))
))

fit &lt;-<span class="st"> </span><span class="kw">stan_glm</span>(clot_time ~<span class="st"> </span>log_plasma *<span class="st"> </span>lot_id, <span class="dt">data =</span> clotting2, <span class="dt">family =</span> Gamma, 
                <span class="dt">prior_intercept =</span> <span class="kw">normal</span>(<span class="dv">0</span>,<span class="dv">1</span>), <span class="dt">prior =</span> <span class="kw">normal</span>(<span class="dv">0</span>,<span class="dv">1</span>),
                <span class="dt">chains =</span> CHAINS, <span class="dt">cores =</span> CORES, <span class="dt">seed =</span> SEED)</code></pre></div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">print</span>(fit, <span class="dt">digits =</span> <span class="dv">3</span>)</code></pre></div>
<p>In the output above, the estimate reported for <code>shape</code> is for the shape parameter of the Gamma distribution. The <em>reciprocal</em> of the shape parameter can be interpreted similarly to what <code>summary.glm</code> refers to as the dispersion parameter.</p>
</div>
<div id="references" class="section level1">
<h1>References</h1>
<p>Gelman, A. and Hill, J. (2007). <em>Data Analysis Using Regression and Multilevel/Hierarchical Models.</em> Cambridge University Press, Cambridge, UK.</p>
<p>McCullagh, P. and Nelder, J. A. (1989). <em>Generalized Linear Models.</em> Chapman and Hall/CRC Press, New York.</p>
</div>



<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
