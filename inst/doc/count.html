<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8">
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />

<meta name="author" content="Jonah Gabry and Ben Goodrich" />

<meta name="date" content="2015-01-06" />

<title>Estimating Generalized Linear Models for Count Data with rstanarm</title>

<script src="count_files/jquery-1.11.0/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="count_files/bootstrap-3.3.1/css/bootstrap.min.css" rel="stylesheet" />
<script src="count_files/bootstrap-3.3.1/js/bootstrap.min.js"></script>
<script src="count_files/bootstrap-3.3.1/shim/html5shiv.min.js"></script>
<script src="count_files/bootstrap-3.3.1/shim/respond.min.js"></script>

<style type="text/css">code{white-space: pre;}</style>
<link rel="stylesheet"
      href="count_files/highlight/default.css"
      type="text/css" />
<script src="count_files/highlight/highlight.js"></script>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>
<script type="text/javascript">
if (window.hljs && document.readyState && document.readyState === "complete") {
   window.setTimeout(function() {
      hljs.initHighlighting();
   }, 0);
}
</script>



</head>

<body>

<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img { 
  max-width:100%; 
  height: auto; 
}
</style>
<div class="container-fluid main-container">


<div id="header">
<h1 class="title">Estimating Generalized Linear Models for Count Data with rstanarm</h1>
<h4 class="author"><em>Jonah Gabry and Ben Goodrich</em></h4>
<h4 class="date"><em>01/06/2015</em></h4>
</div>

<div id="TOC">
<ul>
<li><a href="#introduction">Introduction</a></li>
<li><a href="#likelihood">Likelihood</a></li>
<li><a href="#priors">Priors</a></li>
<li><a href="#posterior">Posterior</a></li>
<li><a href="#poisson-and-negative-binomial-regression-example">Poisson and Negative Binomial Regression Example</a></li>
<li><a href="#references">References</a></li>
</ul>
</div>

<!--
%\VignetteEngine{knitr::rmarkdown}
%\VignetteIndexEntry{stan_glm: GLMs for Count Data}
-->
<div id="introduction" class="section level1">
<h1>Introduction</h1>
<p>This vignette explains how to estimate generalized linear models (GLMs) for count data using the <code>stan_glm</code> function in the <strong>rstanarm</strong> package.</p>
<p>The four steps of a Bayesian analysis are</p>
<ol style="list-style-type: decimal">
<li>Specify a joint distribution for the outcome(s) and all the unknowns, which typically takes the form of a marginal prior distribution for the unknowns multiplied by a likelihood for the outcome(s) conditional on the unknowns. This joint distribution is proportional to a posterior distribution of the unknowns conditional on the observed data</li>
<li>Draw from posterior distribution using Markov Chain Monte Carlo (MCMC).</li>
<li>Evaluate how well the model fits the data and possibly revise the model.</li>
<li>Draw from the posterior predictive distribution of the outcome(s) given interesting values of the predictors in order to visualize how a manipulation of a predictor affects (a function of) the outcome(s).</li>
</ol>
<p>Steps 3 and 4 are covered in more depth by the vignette entitled “How to Use the <strong>rstanarm</strong> Package”. This vignette focuses on Step 1 for Poisson and negative binomial regression models using the <code>stan_glm</code> function.</p>
</div>
<div id="likelihood" class="section level1">
<h1>Likelihood</h1>
<p>If the outcome for a single observation <span class="math inline">\(y\)</span> is assumed to follow a Poisson distribution, the likelihood for one observation can be written as a conditionally Poisson PMF</p>
<p><span class="math display">\[\tfrac{1}{y!} \lambda^y e^{-\lambda},\]</span></p>
<p>where <span class="math inline">\(\lambda = E(y | \mathbf{x}) = g^{-1}(\eta)\)</span> and <span class="math inline">\(\eta = \alpha + \mathbf{x}^\top \boldsymbol{\beta}\)</span> is a linear predictor. For the Poisson distribution it is also true that <span class="math inline">\(\lambda = Var(y | \mathbf{x})\)</span>, i.e. the mean and variance are both <span class="math inline">\(\lambda\)</span>. Later in this vignette we also show how to estimate a negative binomial regression, which relaxes this assumption of equal conditional mean and variance of <span class="math inline">\(y\)</span>.</p>
<p>Because the rate parameter <span class="math inline">\(\lambda\)</span> must be positive, for a Poisson GLM the <em>link</em> function <span class="math inline">\(g\)</span> maps between the positive real numbers <span class="math inline">\(\mathbb{R}^+\)</span> (the support of <span class="math inline">\(\lambda\)</span>) and the set of all real numbers <span class="math inline">\(\mathbb{R}\)</span>. When applied to a linear predictor <span class="math inline">\(\eta\)</span> with values in <span class="math inline">\(\mathbb{R}\)</span>, the inverse link function <span class="math inline">\(g^{-1}(\eta)\)</span> therefore returns a positve real number.</p>
<p>Although other link functions are possible, the canonical link function for a Poisson GLM is the log link <span class="math inline">\(g(x) = \ln{(x)}\)</span>. With the log link, the inverse link function is simply the exponential function and the likelihood for a single observation becomes</p>
<p><span class="math display">\[\frac{g^{-1}(\eta)^y}{y!} e^{-g^{-1}(\eta)} = 
\frac{e^{\eta y}}{y!} e^{-e^\eta}.\]</span></p>
</div>
<div id="priors" class="section level1">
<h1>Priors</h1>
<p>A full Bayesian analysis requires specifying prior distributions <span class="math inline">\(f(\alpha)\)</span> and <span class="math inline">\(f(\boldsymbol{\beta})\)</span> for the intercept and vector of regression coefficients. When using <code>stan_glm</code>, these distributions can be set using the <code>prior_intercept</code> and <code>prior</code> arguments. The <code>stan_glm</code> function supports a variety of prior distributions, which are explained in the <strong>rstanarm</strong> documentation (<code>help(priors, package = 'rstanarm')</code>).</p>
<p>As an example, suppose we have <span class="math inline">\(K\)</span> predictors and believe — prior to seeing the data — that <span class="math inline">\(\alpha, \beta_1, \dots, \beta_K\)</span> are as likely to be positive as they are to be negative, but are highly unlikely to be far from zero. These beliefs can be represented by normal distributions with mean zero and a small scale (standard deviation). To give <span class="math inline">\(\alpha\)</span> and each of the <span class="math inline">\(\beta\)</span>s this prior (with a scale of 1, say), in the call to <code>stan_glm</code> we would include the arguments <code>prior_intercept = normal(0,1)</code> and <code>prior = normal(0,1)</code>.</p>
<p>If, on the other hand, we have less a priori confidence that the parameters will be close to zero then we could use a larger scale for the normal distribution and/or a distribution with heavier tails than the normal like the Student t distribution. <strong>Step 1</strong> in the “How to Use the <strong>rstanarm</strong> Package” vignette discusses one such example.</p>
</div>
<div id="posterior" class="section level1">
<h1>Posterior</h1>
<p>With independent prior distributions, the joint posterior distribution for <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\boldsymbol{\beta}\)</span> in the Poisson model is proportional to the product of the priors and the <span class="math inline">\(N\)</span> likelihood contributions:</p>
<p><span class="math display">\[f\left(\alpha,\boldsymbol{\beta} | \mathbf{y},\mathbf{X}\right) \propto
  f\left(\alpha\right) \times \prod_{k=1}^K f\left(\beta_k\right) \times
  \prod_{i=1}^N {
  \frac{g^{-1}(\eta_i)^{y_i}}{y_i!} e^{-g^{-1}(\eta_i)}}.\]</span></p>
<p>This is posterior distribution that <code>stan_glm</code> will draw from when using MCMC.</p>
</div>
<div id="poisson-and-negative-binomial-regression-example" class="section level1">
<h1>Poisson and Negative Binomial Regression Example</h1>
<p>This example comes from Chapter 8.3 of <a href="http://www.stat.columbia.edu/~gelman/arm/">Gelman and Hill (2007)</a>.</p>
<p>We want to make inferences about the efficacy of a certain pest management system at reducing the number of roaches in urban apartments. Here is how Gelman and Hill describe the experiment (pg. 161):</p>
<blockquote>
<p>[…] the treatment and control were applied to 160 and 104 apartments, respectively, and the outcome measurement <span class="math inline">\(y_i\)</span> in each apartment <span class="math inline">\(i\)</span> was the number of roaches caught in a set of traps. Different apartments had traps for different numbers of days […]</p>
</blockquote>
<p>In addition to an intercept, the regression predictors for the model are the pre-treatment number of roaches <code>roach1</code>, the treatment indicator <code>treatment</code>, and a variable indicating whether the apartment is in a building restricted to elderly residents <code>senior</code>. Because the number of days for which the roach traps were used is not the same for all apartments in the sample, we include it as an exposure, which slightly changes the model described in the <strong>Likelihood</strong> section above in that the rate parameter <span class="math inline">\(\lambda_i = exp(\eta_i)\)</span> is multiplied by the exposure <span class="math inline">\(u_i\)</span> giving us <span class="math inline">\(y_i \sim Poisson(u_i \lambda_i)\)</span>. This is equivalent to adding <span class="math inline">\(\ln{(u_i)}\)</span> to the linear predictor <span class="math inline">\(\eta_i\)</span> and it can be specified using the <code>offset</code> argument to <code>stan_glm</code>:</p>
<pre class="r"><code>library(rstanarm)

# Load data and rescale
roaches &lt;- read.csv(file.path(&quot;data&quot;, &quot;roaches.csv.xz&quot;))
roaches$roach1 &lt;- roaches$roach1 / 100
# Estimate original model
glm1 &lt;- glm(y ~ roach1 + treatment + senior, data = roaches,
            family = poisson, offset = log(exposure2))
# Estimate Bayesian version with stan_glm
stan_glm1 &lt;- stan_glm(y ~ roach1 + treatment + senior, data = roaches, 
                      family = poisson, offset = log(exposure2), 
                      prior = normal(0,2.5), prior_intercept = normal(0,5),
                      chains = CHAINS, cores = CORES, seed = SEED)</code></pre>
<p>The <code>formula</code>, <code>data</code>, <code>family</code>, and <code>offset</code> arguments to <code>stan_glm</code> can be specified in exactly the same way as for <code>glm</code>. The <code>poisson</code> family function defaults to using the log link, but to write code readable to someone not familiar with the defaults we should be explicit and use <code>family = poisson(link = &quot;log&quot;)</code>.</p>
<p>We’ve also specified some optional arguments. The <code>chains</code> argument controls how many Markov chains are executed, the <code>cores</code> argument controls the number of cores utilized by the computer when fitting the model. We also provided a seed so that we have the option to deterministically reproduce these results at any time. The <code>stan_glm</code> function has many other optional arguments that allow for more user control over the way estimation is performed. The documentation for <code>stan_glm</code> has more information about these controls as well as other topics related to GLM estimation.</p>
<p>Here are the point estimates and uncertainties from the <code>glm</code> fit and <code>stan_glm</code> fit, which we see are nearly identical:</p>
<pre class="r"><code>round(rbind(glm = coef(glm1), stan_glm = coef(stan_glm1)), 
      digits = 2)</code></pre>
<pre><code>         (Intercept) roach1 treatment senior
glm             3.09    0.7     -0.52  -0.38
stan_glm        3.09    0.7     -0.52  -0.38</code></pre>
<pre class="r"><code>round(rbind(glm = summary(glm1)$coefficients[, &quot;Std. Error&quot;], stan_glm = se(stan_glm1)), 
      digits = 3)</code></pre>
<pre><code>         (Intercept) roach1 treatment senior
glm            0.021  0.009     0.025  0.033
stan_glm       0.021  0.009     0.024  0.033</code></pre>
<p>(Note: the dataset we have is slightly different from the one used in Gelman and Hill (2007), which leads to slightly different parameter estimates than those shown in the book even when copying the <code>glm</code> call verbatim. Also, we have rescaled the <code>roach1</code> predictor. For the purposes of this example, the actual estimates are less important than the process.)</p>
<p>Gelman and Hill next show how to compare the observed data to replicated datasets from the model to check the quality of the fit. Here we don’t show the original code used by Gelman and Hill because it’s many lines, requiring several loops and some care to get the matrix multiplications right (see pg. 161-162). On the other hand, the <strong>rstanarm</strong> package makes this easy. We can generate replicated datasets with a single line of code using the <code>posterior_predict</code> function:</p>
<pre class="r"><code>yrep &lt;- posterior_predict(stan_glm1)</code></pre>
<p>By default <code>posterior_predict</code> will generate a dataset for each set of parameter draws from the posterior distribution. That is, <code>yrep</code> will be an <span class="math inline">\(S \times N\)</span> matrix, where <span class="math inline">\(S\)</span> is the size of the posterior sample and <span class="math inline">\(N\)</span> is the number of data points. Each row of <code>yrep</code> represents a full dataset generated from the posterior predictive distribution. For more about the importance of the <code>posterior_predict</code> function, see the “How to Use the <strong>rstanarm</strong> Package” vignette.</p>
<p>Gelman and Hill take the simulated datasets and for each of them compute the proportion of zeros and compare to the observed proportion of in the original data. We can do this easily using the <code>pp_check</code> function, which generates graphical comparisons of the data <code>y</code> and replicated datasets <code>yrep</code>.</p>
<pre class="r"><code>prop_zero &lt;- function(y) mean(y == 0)
(prop_zero_test1 &lt;- pp_check(stan_glm1, check = &quot;test&quot;, test = &quot;prop_zero&quot;))</code></pre>
<p><img src="SVGs/ROACHES-plot-pp_check1-1.svg" title="" alt="" width="384" style="display: block; margin: auto;" /></p>
<p>The value of the test statistic (in this case the proportion of zeros) computed from the sample <code>y</code> is the vertical blue line. More than 30% of these observations are zeros, whereas the replicated datasets all contain less than 1% zeros. This is a sign that we should consider a model that more accurately accounts for the large proportion of zeros in the data. Gelman and Hill show how we can do this using an overdispered Poisson regression. To illustrate the use of a different <code>stan_glm</code> model, here we will instead try <a href="https://en.wikipedia.org/wiki/Negative_binomial_distribution">negative binomial</a> regression, which is also used for overdispersed or zero-inflated count data. The negative binomial distribution allows the (conditional) mean and variance of <span class="math inline">\(y\)</span> to differ unlike the Poisson distribution. To fit the negative binomial model can either use the <code>stan_glm.nb</code> function or, equivalently, change the <code>family</code> we specify in the call to <code>stan_glm</code> to <code>neg_binomial_2</code> instead of <code>poisson</code>. To do the latter we can just use <code>update</code>:</p>
<pre class="r"><code>stan_glm2 &lt;- update(stan_glm1, family = neg_binomial_2) </code></pre>
<p>We now use <code>pp_check</code> again, this time to check the proportion of zeros in the replicated datasets under the negative binomial model:</p>
<pre class="r"><code>library(gridExtra)
prop_zero_test2 &lt;- pp_check(stan_glm2, check = &quot;test&quot;, test = &quot;prop_zero&quot;)
# Show graphs for Poisson and negative binomial side by side
grid.arrange(prop_zero_test1 + ggtitle(&quot;Poisson&quot;), 
             prop_zero_test2 + ggtitle(&quot;Negative Binomial&quot;), 
             ncol = 2)</code></pre>
<p><img src="SVGs/ROACHES-plot-pp_check2-1.svg" title="" alt="" width="768" style="display: block; margin: auto;" /></p>
<p>This is a much better fit, as the proportion of zeros in the data falls nicely near the center of the distribution of the proportion of zeros among the replicated datasets. The observed proportion of zeros is quite plausible under this model.</p>
<p>We could have also made these plots manually without using the <code>pp_check</code> function because we have the <code>yrep</code> datasets created by <code>posterior_predict</code>. The <code>pp_check</code> function takes care of this for us, but <code>yrep</code> can be used directly to carry out other posterior predictive checks that aren’t automated by <code>pp_check</code>.</p>
<p>When we comparing the models using the <strong>loo</strong> package we also see a clear preference for the negative binomial model:</p>
<pre class="r"><code>loo1 &lt;- loo(stan_glm1)
loo2 &lt;- loo(stan_glm2)
compare(loo1, loo2)</code></pre>
<pre><code>elpd_diff        se   weight1   weight2 
   5359.3     710.3       0.0       1.0 </code></pre>
<p>The weight of 1 assigned to the negative binomial is not surprising given the better fit we’ve already observed from the posterior predictive checks.</p>
</div>
<div id="references" class="section level1">
<h1>References</h1>
<p>Gelman, A. and Hill, J. (2007). <em>Data Analysis Using Regression and Multilevel/Hierarchical Models.</em> Cambridge University Press, Cambridge, UK.</p>
</div>


</div>

<script>

// add bootstrap table styles to pandoc tables
$(document).ready(function () {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
});

</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
