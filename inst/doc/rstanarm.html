<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8">
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />

<meta name="author" content="Jonah Gabry and Ben Goodrich" />

<meta name="date" content="2015-01-06" />

<title>How to Use the rstanarm Package</title>

<script src="rstanarm_files/jquery-1.11.0/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="rstanarm_files/bootstrap-3.3.1/css/bootstrap.min.css" rel="stylesheet" />
<script src="rstanarm_files/bootstrap-3.3.1/js/bootstrap.min.js"></script>
<script src="rstanarm_files/bootstrap-3.3.1/shim/html5shiv.min.js"></script>
<script src="rstanarm_files/bootstrap-3.3.1/shim/respond.min.js"></script>

<style type="text/css">code{white-space: pre;}</style>
<link rel="stylesheet"
      href="rstanarm_files/highlight/default.css"
      type="text/css" />
<script src="rstanarm_files/highlight/highlight.js"></script>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>
<script type="text/javascript">
if (window.hljs && document.readyState && document.readyState === "complete") {
   window.setTimeout(function() {
      hljs.initHighlighting();
   }, 0);
}
</script>



</head>

<body>

<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img { 
  max-width:100%; 
  height: auto; 
}
</style>
<div class="container-fluid main-container">


<div id="header">
<h1 class="title">How to Use the rstanarm Package</h1>
<h4 class="author"><em>Jonah Gabry and Ben Goodrich</em></h4>
<h4 class="date"><em>01/06/2015</em></h4>
</div>

<div id="TOC">
<ul>
<li><a href="#introduction">Introduction</a></li>
<li><a href="#step-1-specify-a-posterior-distribution">Step 1: Specify a posterior distribution</a><ul>
<li><a href="#note-on-prior-beliefs-and-default-priors">Note on “prior beliefs” and default priors</a></li>
</ul></li>
<li><a href="#step-2-draw-from-the-posterior-distribution">Step 2: Draw from the posterior distribution</a></li>
<li><a href="#step-3-criticize-the-model">Step 3: Criticize the model</a></li>
<li><a href="#step-4-analyze-manipulations-of-predictors">Step 4: Analyze manipulations of predictors</a></li>
<li><a href="#troubleshooting">Troubleshooting</a><ul>
<li><a href="#markov-chains-did-not-converge">Markov chains did not converge</a></li>
<li><a href="#divergent-transitions">Divergent transitions</a></li>
<li><a href="#maximum-treedepth-exceeded">Maximum treedepth exceeded</a></li>
</ul></li>
<li><a href="#conclusion">Conclusion</a></li>
</ul>
</div>

<!--
%\VignetteEngine{knitr::rmarkdown}
%\VignetteIndexEntry{How to Use the rstanarm Package}
-->
<div id="introduction" class="section level1">
<h1>Introduction</h1>
<p>This vignette provides an <em>overview</em> of how to use the functions in the <strong>rstanarm</strong> package that focuses on commonalities. The other <strong>rstanarm</strong> vignettes go into the particularities of each of the individual model-estimating functions.</p>
<p>The goal of the <strong>rstanarm</strong> package is to make Bayesian estimation <em>routine</em> for the most common regression models that applied researchers use. This will enable researchers to avoid the counter-intuitiveness of the frequentist approach to probability and statistics with only minimal changes to their existing R scripts.</p>
<p>The four steps of a Bayesian analysis are</p>
<ol style="list-style-type: decimal">
<li>Specify a joint distribution for the outcome(s) and all the unknowns, which typically takes the form of a marginal prior distribution for the unknowns multiplied by a likelihood for the outcome(s) conditional on the unknowns. This joint distribution is proportional to a posterior distribution of the unknowns conditional on the observed data</li>
<li>Draw from posterior distribution using Markov Chain Monte Carlo (MCMC).</li>
<li>Evaluate how well the model fits the data and possibly revise the model.</li>
<li>Draw from the posterior predictive distribution of the outcome(s) given interesting values of the predictors in order to visualize how a manipulation of a predictor affects (a function of) the outcome(s).</li>
</ol>
<p>Step 1 is necessarily model-specific and is covered in more detail in the other vignettes that cover specific forms of the marginal prior distribution and likelihood of the outcome. It is somewhat more involved than the corresponding first step of a frequentist analysis, which only requires that the likelihood of the outcome be specified. However, the default priors in the <strong>rstanarm</strong> package should work well in the majority of cases. Steps 2, 3, and 4 are the focus of this vignette because they are largely not specific to how the joint distribution in Step 1 is specified.</p>
<p>The key concept in Step 3 and Step 4 is the posterior predictive distribution, which is the distribution of the outcome implied by the model after having used the observed data to update our beliefs about the unknown parameters. Frequentists, by definition, have no posterior predictive distribution and frequentist predictions are subtly different from what applied researchers seek. Maximum likelihood estimates do <em>not</em> condition on the observed outcome data and so the uncertainty in the estimates pertains to the variation in the sampling distribution of the estimator, i.e. the distribution of estimates that would occur if we could repeat the process of drawing a random sample from a well-defined population and apply the estimator to each sample. It is possible to construct a distribution of predictions under the frequentist paradigm but it evokes the hypothetical of repeating the process of drawing a random sample, applying the estimator each time, and generating point predictions of the outcome. In contrast, the posterior predictive distribution conditions on the observed outcome data in hand to update beliefs about the unknowns and the variation in the resulting distribution of predictions reflects the remaining uncertainty in our beliefs about the unknowns.</p>
</div>
<div id="step-1-specify-a-posterior-distribution" class="section level1">
<h1>Step 1: Specify a posterior distribution</h1>
<p>For the sake of discussion, we need some posterior distribution to draw from. We will utilize an example from the <strong>HSAUR3</strong> package by Brian S. Everitt and Torsten Hothorn, which is used in their 2014 book <em>A Handbook of Statistical Analyses Using R (3rd Edition)</em> (Chapman &amp; Hall / CRC). This book is frequentist in nature and we will show how to obtain the corresponding Bayesian results.</p>
<p>The model in section 6.3.2 pertains to whether a survey respondent agrees or disagrees with a conservative statement about the role of women in society, which is modeled as a function of the gender and education of the respondents. The posterior distribution — with independent priors — can be written as <span class="math display">\[f\left(\alpha,\beta_1,\beta_2|\mathbf{y},\mathbf{X}\right) \propto
  f\left(\alpha\right) f\left(\beta_1\right) f\left(\beta_2\right) \times
  \prod_{i=1}^J {
  g^{-1}\left(\eta_i\right)^{y_i} 
  \left(1 - g^{-1}\left(\eta_i\right)\right)^{n_i-y_i}},\]</span> where <span class="math inline">\(\eta_i = \alpha + \beta_1 \mbox{education}_i + \beta_2 \mbox{Female}_i\)</span> is the linear predictor and a function of an intercept <span class="math inline">\(\left(\alpha\right)\)</span>, a coefficient on the years of education <span class="math inline">\(\left(\beta_1\right)\)</span>, and an intercept-shift <span class="math inline">\(\left(\beta_2\right)\)</span> for the case where the respondent is female. These data are organized such that <span class="math inline">\(y_i\)</span> is the number of respondents who agree with the statement that have the same level of education and the same gender, and <span class="math inline">\(n_i - y_i\)</span> is the number of such people who disagree with the statement. The inverse link function, <span class="math inline">\(p = g^{-1}\left(\eta_i \right)\)</span>, for a binomial likelihood can be one of several Cumulative Distribution Functions (CDFs) but in this case is the standard logistic CDF, <span class="math inline">\(g^{-1}\left(\eta_i \right)=\frac{1}{1 + e^{-\eta_i}}\)</span>.</p>
<p>Suppose we believe — prior to seeing the data — that <span class="math inline">\(\alpha\)</span>, <span class="math inline">\(\beta_1\)</span>, and <span class="math inline">\(\beta_2\)</span> are probably close to zero, are as likely to be positive as they are to be negative, but have a small chance of being quite far from zero. These beliefs can be represented by Student t distributions with a few degrees of freedom in order to produce moderately heavy tails. In particular, we will specify seven degrees of freedom. Note that these purported beliefs may well be more skeptical than your actual beliefs, which are probably that women and people with more education have less conservative societal views.</p>
<div id="note-on-prior-beliefs-and-default-priors" class="section level3">
<h3>Note on “prior beliefs” and default priors</h3>
<p>In this vignette we use the term “prior beliefs” to refer in generality to the information content of the prior distribution (conditional on the model). Sometimes previous research on the topic of interest motivates beliefs about model parameters, but other times such work may not exist or several studies may make contradictory claims. Regardless, we nearly always have <em>some</em> knowledge that should be reflected in our choice of prior distributions. For example, no one believes a logistic regression coefficient will be greater than five in absoluate value if the predictors are scaled reasonably. You may also have seen examples of so-called “non-informative” (or “vague”, “diffuse”, etc.) priors like a normal distribution with a variance of 1000. When data are reasonably scaled, these priors are almost always a bad idea for various reasons (they give non-trivial weight to extreme values, reduce computational efficiency, etc). The default priors in <strong>rstanarm</strong> are designed to be <em>weakly informative</em>, by which we mean that they avoid placing unwarranted prior weight on nonsensical parameter values and provide some regularization to avoid overfitting, but also do allow for extreme values if warranted by the data. If additional information is available, the weakly informative defaults can be replaced with more informative priors.</p>
</div>
</div>
<div id="step-2-draw-from-the-posterior-distribution" class="section level1">
<h1>Step 2: Draw from the posterior distribution</h1>
<p>The likelihood for the sample is just the product over the <span class="math inline">\(J\)</span> groups of <span class="math display">\[g^{-1}\left(\eta_i \right)^{y_i} 
  \left(1 - g^{-1}\left(\eta_i \right)\right)^{n_i-y_i},\]</span> which can be maximized over <span class="math inline">\(\alpha\)</span>, <span class="math inline">\(\beta_1\)</span>, and <span class="math inline">\(\beta_2\)</span> to obtain frequentist estimates by calling</p>
<pre class="r"><code>data(&quot;womensrole&quot;, package = &quot;HSAUR3&quot;)
womensrole$total &lt;- womensrole$agree + womensrole$disagree
womensrole_glm_1 &lt;- glm(cbind(agree, disagree) ~ education + gender,
                        data = womensrole, family = binomial(link = &quot;logit&quot;))
round(coef(summary(womensrole_glm_1)), 3)</code></pre>
<pre><code>             Estimate Std. Error z value Pr(&gt;|z|)
(Intercept)     2.509      0.184  13.646    0.000
education      -0.271      0.015 -17.560    0.000
genderFemale   -0.011      0.084  -0.136    0.892</code></pre>
<p>The p-value for the null hypothesis that <span class="math inline">\(\beta_1 = 0\)</span> is very small, while the p-value for the null hypothesis that <span class="math inline">\(\beta_2 = 0\)</span> is very large. However, frequentist p-values are awkward because they do not pertain to the probability that a scientific hypothesis is true but rather to the probability of observing a <span class="math inline">\(z\)</span>-statistic that is so large (in magnitude) if the null hypothesis were true. The desire to make probabalistic statements about a scientific hypothesis is one reason why many people are drawn to the Bayesian approach.</p>
<p>A model with the same likelihood but Student t priors with seven degrees of freedom can be specified using the <strong>rstanarm</strong> package in a similar way by prepending <code>stan_</code> to the <code>glm</code> call and specifying priors (and optionally the number of cores on your computer to utilize):</p>
<pre class="r"><code>library(rstanarm)
womensrole_bglm_1 &lt;- stan_glm(cbind(agree, disagree) ~ education + gender,
                              data = womensrole,
                              family = binomial(link = &quot;logit&quot;), 
                              prior = student_t(df = 7), 
                              prior_intercept = student_t(df = 7),
                              chains = CHAINS, cores = CORES, seed = SEED)
womensrole_bglm_1</code></pre>
<pre><code>stan_glm(formula = cbind(agree, disagree) ~ education + gender, 
    family = binomial(link = &quot;logit&quot;), data = womensrole, chains = CHAINS, 
    cores = CORES, seed = SEED, prior = student_t(df = 7), prior_intercept = student_t(df = 7))

Estimates:
             Median MAD_SD
(Intercept)   2.5    0.2  
education    -0.3    0.0  
genderFemale  0.0    0.1  

Sample avg. posterior predictive 
distribution of y (X = xbar):
         Median MAD_SD
mean_PPD 24.2    0.8  </code></pre>
<p>As can be seen, the “Bayesian point estimates” — which are represented by the posterior medians — are very similar to the maximum likelihood estimates. Frequentists would ask whether the point estimate is greater in magnitude than double the estimated standard deviation of the sampling distribution. But here we simply have estimates of the standard deviation of the marginal posterior distributions, which are based on a scaling of the Median Absolute Deviation (MAD) from the posterior medians to obtain a robust estimator of the posterior standard deviation. In addition, we can use the <code>posterior_interval</code> function to obtain a Bayesian uncertainty interval for <span class="math inline">\(\beta_1\)</span>:</p>
<pre class="r"><code>ci95 &lt;- posterior_interval(womensrole_bglm_1, prob = 0.95, pars = &quot;education&quot;)
round(ci95, 2)</code></pre>
<pre><code>          2.5% 97.5%
education -0.3 -0.24</code></pre>
<p>Unlike frequentist confidence intervals — which are <em>not</em> interpretable in terms of post-data probabilities — the Bayesian uncertainty interval indicates we believe after seeing the data that there is a <span class="math inline">\(0.95\)</span> probability that <span class="math inline">\(\beta_2\)</span> is between -0.3 and -0.24. Alternatively, we could say that there is essentially zero probability that <span class="math inline">\(\beta_2 &gt; 0\)</span>, although frequentists cannot make such a claim coherently.</p>
<p>Many of the postestimation methods that are available for a model that is estimated by <code>glm</code> are also available for a model that is estimated by <code>stan_glm</code>. For example,</p>
<pre class="r"><code>cbind(Median = coef(womensrole_bglm_1), MAD_SD = se(womensrole_bglm_1))</code></pre>
<pre><code>                  Median     MAD_SD
(Intercept)   2.50553917 0.18172590
education    -0.27038917 0.01573899
genderFemale -0.01080511 0.09050671</code></pre>
<pre class="r"><code>summary(residuals(womensrole_bglm_1)) # not deviance residuals</code></pre>
<pre><code>      Min.    1st Qu.     Median       Mean    3rd Qu.       Max. 
-0.3060000 -0.0365000 -0.0037500  0.0002338  0.0662200  0.2818000 
      NA&#39;s 
         1 </code></pre>
<pre class="r"><code>cov2cor(vcov(womensrole_bglm_1))</code></pre>
<pre><code>             (Intercept)   education genderFemale
(Intercept)    1.0000000 -0.93354581  -0.23343542
education     -0.9335458  1.00000000  -0.04282036
genderFemale  -0.2334354 -0.04282036   1.00000000</code></pre>
<p><strong>rstanarm</strong> does provide a <code>confint</code> method, although it is reserved for computing confidence intervals in the case that the user elects to estimate a model by (penalized) maximum likelihood. When using full Bayesian inference (the <strong>rstanarm</strong> default) or approximate Bayesian inference the <code>posterior_interval</code> function should be used to obtain Bayesian uncertainty intervals.</p>
</div>
<div id="step-3-criticize-the-model" class="section level1">
<h1>Step 3: Criticize the model</h1>
<p>The <code>launch_shinystan</code> function in the <strong>shinystan</strong> package provides almost all the tools you need to visualize the posterior distribution and diagnose any problems with the Markov chains. In this case, the results are fine and to verify that, you can call</p>
<pre class="r"><code>launch_shinystan(womensrole_bglm_1)</code></pre>
<p>which will open a web browser that drives the visualizations.</p>
<p>For the rest of this subsection, we focus on what users can do programatically to evaluate whether a model is adequate. A minimal requirement for a Bayesian estimates is that the model should fit the data that the estimates conditioned on. The key function here is <code>posterior_predict</code>, which can be passed a new <code>data.frame</code> to predict out-of-sample, but in this case is omitted to obtain in-sample posterior predictions:</p>
<pre class="r"><code>y_rep &lt;- posterior_predict(womensrole_bglm_1)
dim(y_rep)</code></pre>
<pre><code>[1] 2000   42</code></pre>
<p>The resulting matrix has rows equal to the number of posterior simulations, which in this case is <span class="math inline">\(2000\)</span> and columns equal to the number of observations in the original dataset, which is <span class="math inline">\(42\)</span> combinations of education and gender. Each element of this matrix is a predicted number of respondents with that value of education and gender who agreed with the survey question and thus should be reasonably close to the observed proportion of agreements in the data. We can create a plot to check this:</p>
<pre class="r"><code>par(mfrow = 1:2, mar = c(5,3.7,1,0) + 0.1, las = 3)
boxplot(sweep(y_rep[,womensrole$gender == &quot;Male&quot;], 2, STATS = 
               womensrole$total[womensrole$gender == &quot;Male&quot;], FUN = &quot;/&quot;), 
        axes = FALSE, main = &quot;Male&quot;, pch = NA,
        xlab = &quot;Years of Education&quot;, ylab = &quot;Proportion of Agrees&quot;)
with(womensrole, axis(1, at = education[gender == &quot;Male&quot;] + 1, 
                      labels = 0:20))
axis(2, las = 1)
with(womensrole[womensrole$gender == &quot;Male&quot;,], 
     points(education + 1,  agree / (agree + disagree), 
            pch = 16, col = &quot;red&quot;))
boxplot(sweep(y_rep[,womensrole$gender == &quot;Female&quot;], 2, STATS = 
          womensrole$total[womensrole$gender == &quot;Female&quot;], FUN = &quot;/&quot;), 
          axes = FALSE, main = &quot;Female&quot;, pch = NA,
        xlab = &quot;Years of Education&quot;, ylab = &quot;&quot;)
with(womensrole, axis(1, at = education[gender == &quot;Female&quot;] + 1,
     labels = 0:20))
with(womensrole[womensrole$gender == &quot;Female&quot;,], 
     points(education + 1,  agree / (agree + disagree), 
            pch = 16, col = &quot;red&quot;))</code></pre>
<div class="figure" style="text-align: center">
<img src="SVGs/criticism-1.svg" alt="Posterior predictive boxplots vs. observed datapoints" width="960" />
<p class="caption">
Posterior predictive boxplots vs. observed datapoints
</p>
</div>
<p>Here the boxplots provide the median, interquartile range, and hinges of the posterior predictive distribution for a given gender and level of education, while the red points represent the corresponding observed data. As can be seen, the model predicts the observed data fairly well for six to sixteen years of education but predicts less well for very low or very high levels of education where there are less data.</p>
<p>Consequently, we might consider a model where education has a quadratic effect on agreement, which is easy to specify using R’s formula-based syntax.</p>
<pre class="r"><code>(womensrole_bglm_2 &lt;- update(womensrole_bglm_1, formula. = . ~ . + I(education^2)))</code></pre>
<pre><code>stan_glm(formula = cbind(agree, disagree) ~ education + gender + 
    I(education^2), family = binomial(link = &quot;logit&quot;), data = womensrole, 
    chains = CHAINS, cores = CORES, seed = SEED, prior = student_t(df = 7), 
    prior_intercept = student_t(df = 7))

Estimates:
               Median MAD_SD
(Intercept)     2.0    0.4  
education      -0.2    0.1  
genderFemale    0.0    0.1  
I(education^2)  0.0    0.0  

Sample avg. posterior predictive 
distribution of y (X = xbar):
         Median MAD_SD
mean_PPD 24.3    0.8  </code></pre>
<p>Frequentists would test the null hypothesis that the coefficient on the squared level of education is zero. Bayesians might ask whether such a model is expected to produce better out-of-sample predictions than a model with only the level of education. The latter question can be answered using leave-one-out cross-validation or the approximation thereof provided by the <code>loo</code> function in the <strong>loo</strong> package, for which a method is provided by the <strong>rstanarm</strong> package.</p>
<pre class="r"><code>loo_bglm_1 &lt;- loo(womensrole_bglm_1)
loo_bglm_2 &lt;- loo(womensrole_bglm_2)</code></pre>
<p>First, we verify that the posterior is not too sensitive to any particular observation in the dataset.</p>
<pre class="r"><code>par(mfrow = 1:2, mar = c(5,3.8,1,0) + 0.1, las = 3)
plot(loo_bglm_1, label_points = TRUE)
plot(loo_bglm_2, label_points = TRUE)</code></pre>
<p><img src="SVGs/loo_plot-1.svg" title="" alt="" width="672" style="display: block; margin: auto;" /></p>
<p>There are only one or two moderate outliers (whose statistics are greater than <span class="math inline">\(0.5\)</span>), which should not have too much of an effect on the resulting model comparison:</p>
<pre class="r"><code>compare(loo_bglm_1, loo_bglm_2)</code></pre>
<pre><code>elpd_diff        se   weight1   weight2 
     -0.6       1.8       0.6       0.4 </code></pre>
<p>In this case, there is little difference in the expected log pointwise deviance between the two models, so we are essentially indifferent between them after taking into account that the second model estimates an additional parameter. The “LOO Information Criterion (LOOIC)”</p>
<pre class="r"><code>loo_bglm_1</code></pre>
<pre><code>Computed from 2000 by 42 log-likelihood matrix

         Estimate   SE
elpd_loo   -104.9  9.5
p_loo         4.2  1.7
looic       209.8 19.0</code></pre>
<p>has the same purpose as the Aikaike Information Criterion (AIC) that is used by frequentists. Both are intended to estimate the expected log predicted density (ELPD) for a new dataset. However, the AIC ignores priors and assumes that the posterior distribution is multivariate normal, whereas the functions from the loo package used here do not assume that the posterior distribution is multivariate normal and integrate over uncertainty in the parameters. This only assumes that any one observation can be omitted without having a major effect on the posterior distribution, which can be judged using the plots above.</p>
</div>
<div id="step-4-analyze-manipulations-of-predictors" class="section level1">
<h1>Step 4: Analyze manipulations of predictors</h1>
<p>Frequentists attempt to interpret the estimates of the model, which is difficult except when the model is linear, has no inverse link function, and contains no interaction terms. Bayesians can avoid this difficulty simply by inspecting the posterior predictive distribution at different levels of the predictors. For example,</p>
<pre class="r"><code>y_rep &lt;- posterior_predict(womensrole_bglm_2, newdata = 
                             data.frame(education = c(12,16),
                                        gender = factor(&quot;Female&quot;, levels = 
                                                       c(&quot;Male&quot;, &quot;Female&quot;))))
y_rep &lt;- with(womensrole, sweep(y_rep, 2, FUN = &quot;/&quot;, STATS = 
                total[gender == &quot;Female&quot; &amp; education %in% c(12,16)]))
summary(apply(y_rep, 1, diff))</code></pre>
<pre><code>     Min.   1st Qu.    Median      Mean   3rd Qu.      Max. 
-0.008432 -0.003373 -0.001686 -0.001242  0.000000  0.015620 </code></pre>
<p>As can be seen, two women — one with a high school degree and another with a college degree — are expected to have about the same probability of agreeing with the survey question. This is largely due to the fact that women with a high school education are already extremely unlikely to agree with the survey question and thus increasing their education to a college degree cannot have a large negative impact on this probability. Thus, to the extent that education makes a difference to the outcome, it is mostly that people with very little education have a considerable probability of agreeing with the survey question.</p>
</div>
<div id="troubleshooting" class="section level1">
<h1>Troubleshooting</h1>
<p>This section provides suggestions for how to proceed when you encounter warning messages generated by the modeling functions in the <strong>rstanarm</strong> package. The example models below are used just for the purposes of concisely demonstrating certain difficulties and possible remedies (we won’t worry about the merit of the models themselves). The references at the end provide more information on the relevant issues.</p>
<div id="markov-chains-did-not-converge" class="section level3">
<h3>Markov chains did not converge</h3>
<p><strong>Recommendation:</strong> run the chains for more iterations. <br></p>
<p>By default, all <strong>rstanarm</strong> modeling functions will run four randomly initialized Markov chains, each for 2000 iterations (including a warmup period of 1000 iterations that is discarded). All chains must converge to the target distribution for inferences to be valid. For most models, the default settings are sufficient, but if you see a warning message about Markov chains not converging, the first thing to try is increasing the number of iterations. This can be done by specifying the <code>iter</code> argument (e.g. <code>iter = 3000</code>).</p>
<p>One way to monitor whether a chain has converged to the equilibrium distribution is to compare its behavior to other randomly initialized chains. This is the motivation for the Gelman and Rubin potential scale reduction statistic Rhat. The Rhat statistic measures the ratio of the average variance of the draws within each chain to the variance of the pooled draws across chains; if all chains are at equilibrium, these will be the same and Rhat will be one. If the chains have not converged to a common distribution, the Rhat statistic will tend to be greater than one.</p>
<p>Gelman and Rubin’s recommendation is that the independent Markov chains be initialized with diffuse starting values for the parameters and sampled until all values for Rhat are below 1.1. When any Rhat values are above 1.1 <strong>rstanarm</strong> will print a warning message like this:</p>
<pre><code>Markov chains did not converge! Do not analyze results! </code></pre>
<p>To illustrate how to check the Rhat values after fitting a model using <strong>rstanarm</strong> we’ll fit two models and run them for different numbers of iterations.</p>
<pre class="r"><code>bad_rhat &lt;- stan_glm(mpg ~ ., data = mtcars, iter = 20, 
                     chains = CHAINS, cores = CORES, seed = SEED)</code></pre>
<pre><code>Warning: Markov chains did not converge! Do not analyze results!</code></pre>
<pre class="r"><code>good_rhat &lt;- update(bad_rhat, iter = 500, 
                    chains = CHAINS, cores = CORES, seed = SEED)</code></pre>
<p>Here the first model leads to the warning message about convergence but the second model does not. Indeed, we can see that not all Rhat values are less than 1.1 for the first model:</p>
<pre class="r"><code>rhat &lt;- summary(bad_rhat)[, &quot;Rhat&quot;]
rhat[rhat &gt; 1.1]</code></pre>
<pre><code>         disp            hp            vs            am          gear 
     1.241645      1.413041      1.155705      1.158465      1.153849 
         carb         sigma      mean_PPD log-posterior 
     1.174211      1.650953      1.789107      2.679556 </code></pre>
<p>Since we didn’t get a warning for the second model we shouldn’t find any parameters with an Rhat greater than 1.1:</p>
<pre class="r"><code>any(summary(good_rhat)[, &quot;Rhat&quot;] &gt; 1.1)</code></pre>
<pre><code>[1] FALSE</code></pre>
<p>Details on the computatation of Rhat and some of its limitations can be found in the chapter of the <a href="http://mc-stan.org/documentation/">Stan Modeling Language User’s Guide and Reference Manual</a>.</p>
</div>
<div id="divergent-transitions" class="section level3">
<h3>Divergent transitions</h3>
<p><strong>Recommendation:</strong> increase the target acceptance rate <code>adapt_delta</code>. <br></p>
<p>Hamiltonian Monte Carlo (HMC), the MCMC algorithm used by <a href="http://mc-stan.org">Stan</a>, works by simulating the evolution of a Hamiltonian system. Stan uses a <a href="https://en.wikipedia.org/wiki/Symplectic_integrator">symplectic integrator</a> to approximate the exact solution of the Hamiltonian dynamics. When the step size parameter is too large relative to the curvature of the log posterior this approximation can diverge and threaten the validity of the sampler. <strong>rstanarm</strong> will print a warning if there are any divergent transitions after the warmup period, in which case the posterior sample may be biased. The recommended method is to increase the <code>adapt_delta</code> parameter – target average proposal acceptance probability in the adaptation – which will in turn reduce the step size. Each of the modeling functions accepts an <code>adapt_delta</code> argument, so to increase <code>adapt_delta</code> you can simply change the value from the default value to a value closer to <span class="math inline">\(1\)</span>. To reduce the frequency with which users need to manually set <code>adapt_delta</code>, the default value depends on the prior distribution used (see <code>help(&quot;adapt_delta&quot;, package = &quot;rstanarm&quot;)</code> for details).</p>
<p>The downside to increasing the target acceptance rate – and, as a consequence, decreasing the step size – is that sampling will tend to be slower. Intuitively, this is because a smaller step size means that more steps are required to explore the posterior distribution. Since the validity of the estimates is not guaranteed if there are any post-warmup divergent transitions, the slower sampling is a minor cost.</p>
</div>
<div id="maximum-treedepth-exceeded" class="section level3">
<h3>Maximum treedepth exceeded</h3>
<p><strong>Recommendation:</strong> increase the maximum allowed treedepth <code>max_treedepth</code>. <br></p>
<p>Configuring the No-U-Turn-Sampler (the variant of HMC used by Stan) involves putting a cap on the depth of the trees that it evaluates during each iteration. This is controlled through a maximum depth parameter <code>max_treedepth</code>. When the maximum allowed tree depth is reached it indicates that NUTS is terminating prematurely to avoid excessively long execution time. If <strong>rstanarm</strong> prints a warning about transitions exceeding the maximum treedepth you should try increasing the <code>max_treedepth</code> parameter using the optional <code>control</code> argument. For example, to increase <code>max_treedepth</code> to 20 (the default used <strong>rstanarm</strong> is 15) you can provide the argument <code>control = list(max_treedepth = 20)</code> to any of the <strong>rstanarm</strong> modeling functions. If you do not see a warning about hitting the maximum treedepth (which is rare), then you do not need to worry.</p>
</div>
</div>
<div id="conclusion" class="section level1">
<h1>Conclusion</h1>
<p>In this vignette, we have gone through the four steps of a Bayesian analysis. The first step — specifying the posterior distribution — varies considerably from one analysis to the next because the likelihood function employed differs depending on the nature of the outcome variable and our prior beliefs about the parameters in the model varies not only from situation to situation but from researcher to researcher. However, given a posterior distribution and given that this posterior distribution can be drawn from using the <strong>rstanarm</strong> package, the remaining steps are conceptually similar across analyses. The key is to draw from the posterior predictive distribution of the outcome, which is the what the model predicts the outcome to be after having updated our beliefs about the unknown parameters with the observed data. Posterior predictive distributions can be used for model checking and for making inferences about how manipulations of the predictors would affect the outcome.</p>
<p>Of course, all of this assumes that you have obtained draws from the posterior distribution faithfully. The functions in the <strong>rstanarm</strong> package will throw warnings if there is evidence that the draws are tainted, and we have discussed some steps to remedy these problems. For the most part, the model-fitting functions in the <strong>rstanarm</strong> package are unlikely to produce many such warnings, but they may appear in more complicated models.</p>
<p>If the posterior distribution that you specify in the first step cannot be sampled from using the <strong>rstanarm</strong> package, then it is often possible to create a hand-written program in the the Stan language so that the posterior distribution can be drawn from using the <strong>rstan</strong> package. See the documentation for the <strong>rstan</strong> package or <a href="http://mc-stan.org" class="uri">http://mc-stan.org</a> for more details about this more advanced useage of Stan. However, many relatively simple models can be fit using the <strong>rstanarm</strong> package without writing any code in the Stan language, which is illustrated for each estimating function in the <strong>rstanarm</strong> package in the other vignettes.</p>
<p><strong>References</strong></p>
<p>Betancourt, M. J., &amp; Girolami, M. (2013). Hamiltonian Monte Carlo for hierarchical models. <a href="http://arxiv.org/abs/1312.0906">arXiv preprint</a>.</p>
<p>Stan Development Team. (2015). <em>Stan modeling language user’s guide and reference manual, Version 2.9.0</em>. <a href="http://mc-stan.org/documentation" class="uri">http://mc-stan.org/documentation</a>. See the ‘Hamiltonian Monte Carlo Sampling’ chapter.</p>
<p>Gelman, A., &amp; Rubin, D. B. (1992). Inference from iterative simulation using multiple sequences. <em>Statistical Science</em>, 7(4), 457 – 472.</p>
<p>Gelman, A., &amp; Shirley, K. (2011). Inference from simulations and monitoring convergence. In S. Brooks, A. Gelman, G. Jones, &amp; X. Meng (Eds.), <em>Handbook of Markov chain Monte Carlo</em>. Boca Raton: Chapman &amp; Hall/CRC.</p>
</div>


</div>

<script>

// add bootstrap table styles to pandoc tables
$(document).ready(function () {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
});

</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
